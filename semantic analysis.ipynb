{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n!pip install openpyxl\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-09T07:59:39.587976Z","iopub.execute_input":"2021-08-09T07:59:39.588726Z","iopub.status.idle":"2021-08-09T07:59:49.369045Z","shell.execute_reply.started":"2021-08-09T07:59:39.588587Z","shell.execute_reply":"2021-08-09T07:59:49.367849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reddit_sample = pd.read_csv('/kaggle/input/donald-trump-comments-on-reddit/reddit_trump.csv', error_bad_lines=False)\ntwitter_sample = pd.read_csv(\"/kaggle/input/twitter/Sentiment.csv\")\n#data = pd.read_csv('file1.csv', error_bad_lines=False)\nreddit_sample = pd.read_csv(\"/kaggle/input/redditsentiments/reddit_sentiment.csv\") \nyoutube_sample = pd.read_csv(\"/kaggle/input/youtubesentiments/youtube_sentiment.csv\")\ngeneric_sample = pd.read_csv(\"/kaggle/input/genericcomments/generic_sentiment.csv\")\n#reddit_comments = pd.read_csv(\"/kaggle/input/genericsentiment/generic_sentiment.csv\")\nreddit_comments = pd.read_excel('/kaggle/input/redditredo/redditcomments.xlsx', index_col=0)\nyoutube_comments = pd.read_csv(\"/kaggle/input/youtuberedo/data.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:20:56.298071Z","iopub.execute_input":"2021-08-09T00:20:56.298597Z","iopub.status.idle":"2021-08-09T00:20:58.110219Z","shell.execute_reply.started":"2021-08-09T00:20:56.298563Z","shell.execute_reply":"2021-08-09T00:20:58.109338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_comments.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:21:24.621411Z","iopub.execute_input":"2021-08-09T00:21:24.621949Z","iopub.status.idle":"2021-08-09T00:21:24.632568Z","shell.execute_reply.started":"2021-08-09T00:21:24.621881Z","shell.execute_reply":"2021-08-09T00:21:24.631558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_sample = pd.read_csv(\"/kaggle/input/redditsentiments/reddit_sentiment.csv\") \nreddit_sample.info()\nyoutube_sample = pd.read_csv(\"/kaggle/input/youtubesentiments/youtube_sentiment.csv\")\nyoutube_sample.info()\ntwitter_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T14:16:37.644142Z","iopub.execute_input":"2021-07-14T14:16:37.644724Z","iopub.status.idle":"2021-07-14T14:16:37.736132Z","shell.execute_reply.started":"2021-07-14T14:16:37.64469Z","shell.execute_reply":"2021-07-14T14:16:37.73483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"youtube_sample = youtube_sample.head(5000)\nyoutube_sample.info()\nreddit_sample.info()\ntwitter_sample = twitter_sample.head(5000)\ntwitter_sample.info()\ngeneric_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T14:37:35.121483Z","iopub.execute_input":"2021-07-14T14:37:35.121855Z","iopub.status.idle":"2021-07-14T14:37:35.166922Z","shell.execute_reply.started":"2021-07-14T14:37:35.121825Z","shell.execute_reply":"2021-07-14T14:37:35.165596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame({\n    'Titter': [76,78,80,79,80],\n    'Reddit': [67,68,62,61,66],\n    'Youtube': [90,91,88,88,88],\n    'Generic': [80,82,61,60,62],\n   }, index=['First ML', 'Second ML', 'MN NB', 'LR', 'SVM'], )\nlines = df.plot.line(title=\"Testing #1 graphical representation\", legend=False)\nlines.set_xlabel(\"Algorithm\")\nlines.set_ylabel(\"Accuracy (%)\")\n#lines.title(\"Testing #1 graphical representation\")\n#df.suptitle(\"Testing #1 graphical representation\")","metadata":{"execution":{"iopub.status.busy":"2021-08-09T03:57:41.015538Z","iopub.execute_input":"2021-08-09T03:57:41.015958Z","iopub.status.idle":"2021-08-09T03:57:41.204598Z","shell.execute_reply.started":"2021-08-09T03:57:41.015922Z","shell.execute_reply":"2021-08-09T03:57:41.203296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Making the generic dataset 1.6k of each","metadata":{}},{"cell_type":"code","source":"\ntwitter_samplet = twitter_sample.head(2000)\ndft = df.head(2000)\nyoutube_commentst = youtube_comments.head(2000)\n\nlist_of_t_c = twitter_samplet['correct_size'].values\nlist_of_t_s = twitter_samplet['sentiment'].values\nlist_of_r_c = dft['correct_size'].values\nlist_of_r_s = dft['comp_score'].values\nlist_of_y_c = youtube_commentst['correct_length'].values\nlist_of_y_s = youtube_commentst['comp_score'].values\nlist_of_g_c = []\nlist_of_g_s = []\n\nfor thing in list_of_t_c:\n    list_of_g_c.append(thing)\nfor thing in list_of_r_c:\n    list_of_g_c.append(thing)\nfor thing in list_of_y_c:\n    list_of_g_c.append(thing)\n\nfor thing in list_of_t_s:\n    list_of_g_s.append(thing)\nfor thing in list_of_r_s:\n    list_of_g_s.append(thing)\nfor thing in list_of_y_s:\n    list_of_g_s.append(thing)    \n\nprint(len(list_of_g_c))\nprint(len(list_of_g_s))\n\ngeneric_sentimentredo = pd.DataFrame(data=list_of_g_c, columns = ['comments'])\ngeneric_sentimentredo['sentiment'] = list_of_g_s\ngeneric_sentimentredo['comments'] = list_of_g_c\n\ngeneric_sentimentredo.to_csv('generic_sentimentredo.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:58:35.551414Z","iopub.execute_input":"2021-08-09T00:58:35.551803Z","iopub.status.idle":"2021-08-09T00:58:35.608045Z","shell.execute_reply.started":"2021-08-09T00:58:35.551768Z","shell.execute_reply":"2021-08-09T00:58:35.607059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generic_sentimentredo.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:59:14.805803Z","iopub.execute_input":"2021-08-09T00:59:14.806262Z","iopub.status.idle":"2021-08-09T00:59:14.827424Z","shell.execute_reply.started":"2021-08-09T00:59:14.806228Z","shell.execute_reply":"2021-08-09T00:59:14.826563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_g_c.append(list_of_t_c)\nlist_of_g_c.append(list_of_r_c)\nlist_of_g_c.append(list_of_y_c)\n\nlist_of_g_s.append(list_of_t_s)\nlist_of_g_s.append(list_of_r_s)\nlist_of_g_s.append(list_of_y_s)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Youtube processing","metadata":{}},{"cell_type":"code","source":"import re\nimport string\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\nyoutube_comments['text'] = youtube_comments['text'].apply(lambda x:clean_text(x))\n\nimport emoji\nimport spacy\nimport stop_words\ns = youtube_comments['text']\nlist_of_list = []\n#print(s)\ndef give_emoji_free_text(text):\n    return emoji.get_emoji_regexp().sub(r'', text)\n\nnlp = spacy.load(\"en_core_web_sm\")\nfor string in s:\n    string = string.lower()\n    string = give_emoji_free_text(string)\n    my_list = \"\"\n    doc = nlp(string)\n    #print(doc)\n   #print('1')\n    for token in doc:\n        if token.text[0:2] == \"rt\":\n            my_list == my_list\n        else:\n            lemm = token.lemma_\n            poss = token.pos_\n            stopp = token.is_stop\n            thisdict = {\n                \"lem\": lemm,\n                \"pos\": poss,\n                \"stop\": stopp\n            }\n            if (thisdict[\"stop\"] == False) & ((thisdict[\"pos\"] == 'PROPN') or (thisdict[\"pos\"] == 'VERB') or (thisdict[\"pos\"] == 'NOUN') or (thisdict[\"pos\"] == 'NUM') or (thisdict[\"pos\"] == 'ADV') or (thisdict[\"pos\"] == 'ADJ')):\n                my_list += thisdict[\"lem\"] + ' '\n            \n    list_of_list.append(my_list)\n\n#print(list_of_list.values)\nyoutube_comments['processing_text'] = list_of_list","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:31:33.525147Z","iopub.execute_input":"2021-08-09T00:31:33.525513Z","iopub.status.idle":"2021-08-09T00:33:07.828738Z","shell.execute_reply.started":"2021-08-09T00:31:33.525482Z","shell.execute_reply":"2021-08-09T00:33:07.827646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_c = youtube_comments['processing_text'].values\nlist_of_r = []\n\nfor comment in list_of_c:\n    cplit = comment.split()\n    if len(cplit) < 3:\n        list_of_r.append('NaN')\n    elif len(cplit) > 27:\n        list_of_r.append(cplit[:27])\n    else:\n        list_of_r.append(comment)\n        \nprint(len(list_of_c))\nprint(len(list_of_r))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:34:58.812785Z","iopub.execute_input":"2021-08-09T00:34:58.813185Z","iopub.status.idle":"2021-08-09T00:34:58.832832Z","shell.execute_reply.started":"2021-08-09T00:34:58.81315Z","shell.execute_reply":"2021-08-09T00:34:58.831944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"youtube_comments['correct_length'] = list_of_r\nyoutube_comments = youtube_comments[youtube_comments['correct_length'].notna()]\nyoutube_comments = youtube_comments[youtube_comments.correct_length != \"NaN\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:35:00.75024Z","iopub.execute_input":"2021-08-09T00:35:00.750604Z","iopub.status.idle":"2021-08-09T00:35:00.769839Z","shell.execute_reply.started":"2021-08-09T00:35:00.750572Z","shell.execute_reply":"2021-08-09T00:35:00.768688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"youtube_sample = youtube_sample[youtube_sample.correct_length != \"NaN\"]","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:53:52.241762Z","iopub.execute_input":"2021-07-12T18:53:52.242088Z","iopub.status.idle":"2021-07-12T18:53:52.248803Z","shell.execute_reply.started":"2021-07-12T18:53:52.242052Z","shell.execute_reply":"2021-07-12T18:53:52.247961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"youtube_comments.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:35:08.830844Z","iopub.execute_input":"2021-08-09T00:35:08.831252Z","iopub.status.idle":"2021-08-09T00:35:08.854561Z","shell.execute_reply.started":"2021-08-09T00:35:08.831219Z","shell.execute_reply":"2021-08-09T00:35:08.853492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_comments = pd.read_excel('/kaggle/input/redditredo/redditcomments.xlsx', index_col=0)\nreddit_comments = reddit_comments[reddit_comments['comments'].notna()]\nreddit_comments.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:21:14.286989Z","iopub.execute_input":"2021-08-09T05:21:14.287381Z","iopub.status.idle":"2021-08-09T05:21:16.278847Z","shell.execute_reply.started":"2021-08-09T05:21:14.287348Z","shell.execute_reply":"2021-08-09T05:21:16.277772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reddit data processing","metadata":{}},{"cell_type":"code","source":"list_of_com = reddit_comments['comments'].values\nprint(len(list_of_com))\ncomplete_list = []\ntemp = []\n#print(len(temp))\n\nfor com in list_of_com:  \n    com = str(com)\n    if com[0:12] == '[+][deleted]': \n        complete_list.append('NaN')\n    elif com[0:9] == 'load more':\n        complete_list.append('NaN')\n    elif com[0:3] == '[–]':\n        complete_list.append('NaN')\n    elif com[0:3] == '[+]':\n        complete_list.append('NaN')\n    else:\n        split_com = com.split()    \n        if len(split_com) > 1:\n            checker = split_com[0]\n            #print(checker[0:9])\n            if checker[0:9] == 'permalink':\n                #print(\"Yes\")\n                if len(temp) == 0:\n                    temp.append(com)\n                else:\n                    complete_list.append(temp)\n                    temp = []\n                    temp.append(com)\n                    #print(\"added new\")\n            else:\n                temp.append(\" \" + com)\n        \n        \nprint(len(complete_list))\n#tempdf = \n#df = df[df.correct_size != \"NaN\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:21:18.3685Z","iopub.execute_input":"2021-08-09T05:21:18.368945Z","iopub.status.idle":"2021-08-09T05:21:18.438843Z","shell.execute_reply.started":"2021-08-09T05:21:18.368883Z","shell.execute_reply":"2021-08-09T05:21:18.437793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comment_list = []\n\nfor com in complete_list:   \n    com = str(com)\n    if com[0:3] == 'NaN':\n        comment_list.append('NaN')\n    #print(com)\n    elif com[2:11] == 'permalink':\n        brack_split_com = com.split('children)')\n        #print(len(brack_split_com))\n        if len(brack_split_com) == 2:\n                comment_list.append(brack_split_com[1])\n        elif len(brack_split_com) == 1:\n            #print(brack_split_com)\n            single_child_split_com = com.split('child)')\n            if len(single_child_split_com) == 2:\n                comment_list.append(single_child_split_com[1])\n \n\nprint(len(comment_list))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:21:29.4406Z","iopub.execute_input":"2021-08-09T05:21:29.441142Z","iopub.status.idle":"2021-08-09T05:21:29.496426Z","shell.execute_reply.started":"2021-08-09T05:21:29.441108Z","shell.execute_reply":"2021-08-09T05:21:29.495253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"redditredo = pd.DataFrame(data=comment_list, columns = ['comments'])\nredditredo = redditredo[redditredo.comments != \"NaN\"]\nredditredo.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:21:33.096322Z","iopub.execute_input":"2021-08-09T05:21:33.0967Z","iopub.status.idle":"2021-08-09T05:21:33.11617Z","shell.execute_reply.started":"2021-08-09T05:21:33.096663Z","shell.execute_reply":"2021-08-09T05:21:33.115128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"redditredo.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:21:41.162558Z","iopub.execute_input":"2021-08-09T05:21:41.162939Z","iopub.status.idle":"2021-08-09T05:21:41.176497Z","shell.execute_reply.started":"2021-08-09T05:21:41.162908Z","shell.execute_reply":"2021-08-09T05:21:41.175017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsid = SentimentIntensityAnalyzer()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:26:22.318936Z","iopub.execute_input":"2021-08-09T00:26:22.319462Z","iopub.status.idle":"2021-08-09T00:26:24.274472Z","shell.execute_reply.started":"2021-08-09T00:26:22.319429Z","shell.execute_reply":"2021-08-09T00:26:24.273199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"redditredo['scores'] = redditredo['comments'].apply(lambda review: sid.polarity_scores(review))\nredditredo['compound']  = redditredo['scores'].apply(lambda score_dict: score_dict['compound'])\nredditredo['comp_score'] = redditredo['compound'].apply(lambda c: 'positive' if c >=0 else 'negative')\n\nredditredo.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:21:51.627406Z","iopub.execute_input":"2021-08-09T05:21:51.627781Z","iopub.status.idle":"2021-08-09T05:21:55.21832Z","shell.execute_reply.started":"2021-08-09T05:21:51.627748Z","shell.execute_reply":"2021-08-09T05:21:55.217517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"youtube_comments['scores'] = youtube_comments['text'].apply(lambda review: sid.polarity_scores(review))\nyoutube_comments['compound']  = youtube_comments['scores'].apply(lambda score_dict: score_dict['compound'])\nyoutube_comments['comp_score'] = youtube_comments['compound'].apply(lambda c: 'positive' if c >=0 else 'negative')\n\nyoutube_comments.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:30:32.458765Z","iopub.execute_input":"2021-08-09T00:30:32.459187Z","iopub.status.idle":"2021-08-09T00:30:34.993835Z","shell.execute_reply.started":"2021-08-09T00:30:32.459155Z","shell.execute_reply":"2021-08-09T00:30:34.992804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport string\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\nredditredo['comments'] = redditredo['comments'].apply(lambda x:clean_text(x))\n\nimport emoji\nimport spacy\nimport stop_words\ns = redditredo['comments']\nlist_of_list = []\n#print(s)\ndef give_emoji_free_text(text):\n    return emoji.get_emoji_regexp().sub(r'', text)\n\nnlp = spacy.load(\"en_core_web_sm\")\nfor string in s:\n    string = string.lower()\n    string = give_emoji_free_text(string)\n    my_list = \"\"\n    doc = nlp(string)\n    #print(doc)\n   #print('1')\n    for token in doc:\n        if token.text[0:2] == \"rt\":\n            my_list == my_list\n        else:\n            lemm = token.lemma_\n            poss = token.pos_\n            stopp = token.is_stop\n            thisdict = {\n                \"lem\": lemm,\n                \"pos\": poss,\n                \"stop\": stopp\n            }\n            if (thisdict[\"stop\"] == False) & ((thisdict[\"pos\"] == 'PROPN') or (thisdict[\"pos\"] == 'VERB') or (thisdict[\"pos\"] == 'NOUN') or (thisdict[\"pos\"] == 'NUM') or (thisdict[\"pos\"] == 'ADV') or (thisdict[\"pos\"] == 'ADJ')):\n                my_list += thisdict[\"lem\"] + ' '\n            \n    list_of_list.append(my_list)\n\n#print(list_of_list.values)\nredditredo['correct_length'] = list_of_list","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:22:08.591855Z","iopub.execute_input":"2021-08-09T05:22:08.592399Z","iopub.status.idle":"2021-08-09T05:23:44.404869Z","shell.execute_reply.started":"2021-08-09T05:22:08.592365Z","shell.execute_reply":"2021-08-09T05:23:44.404122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"redditredo.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:23:52.861476Z","iopub.execute_input":"2021-08-09T05:23:52.862019Z","iopub.status.idle":"2021-08-09T05:23:52.878822Z","shell.execute_reply.started":"2021-08-09T05:23:52.861972Z","shell.execute_reply":"2021-08-09T05:23:52.877558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_c = redditredo['correct_length']\nlist_of_r = []\n\nfor comment in list_of_c:\n    cplit = comment.split()\n    if len(cplit) < 3:\n        list_of_r.append('NaN')\n    elif len(cplit) > 27:\n        list_of_r.append(cplit[:27])\n    else:\n        list_of_r.append(comment)\n        \nprint(len(list_of_c))\nprint(len(list_of_r))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:23:55.928956Z","iopub.execute_input":"2021-08-09T05:23:55.929319Z","iopub.status.idle":"2021-08-09T05:23:55.954388Z","shell.execute_reply.started":"2021-08-09T05:23:55.929288Z","shell.execute_reply":"2021-08-09T05:23:55.952994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"redditredo['correct_size'] = list_of_r","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:24:00.941918Z","iopub.execute_input":"2021-08-09T05:24:00.942285Z","iopub.status.idle":"2021-08-09T05:24:00.951618Z","shell.execute_reply.started":"2021-08-09T05:24:00.942253Z","shell.execute_reply":"2021-08-09T05:24:00.950079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"redditredo = redditredo[redditredo.correct_size != \"NaN\"]\nredditredo.info()\n#twitter_sample = twitter_sample[twitter_sample.sentiment != \"Neutral\"]","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:24:03.095153Z","iopub.execute_input":"2021-08-09T05:24:03.095534Z","iopub.status.idle":"2021-08-09T05:24:03.119293Z","shell.execute_reply.started":"2021-08-09T05:24:03.095499Z","shell.execute_reply":"2021-08-09T05:24:03.118188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:24:06.206793Z","iopub.execute_input":"2021-08-09T05:24:06.207177Z","iopub.status.idle":"2021-08-09T05:24:06.220862Z","shell.execute_reply.started":"2021-08-09T05:24:06.207142Z","shell.execute_reply":"2021-08-09T05:24:06.2198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Twitter data processing","metadata":{}},{"cell_type":"code","source":"twitter_sample.drop(columns=['relevant_yn'], inplace=True)\ntwitter_sample.drop(columns=['relevant_yn_confidence'], inplace=True)\ntwitter_sample.drop(columns=['subject_matter'], inplace=True)\ntwitter_sample.drop(columns=['subject_matter_confidence'], inplace=True)\ntwitter_sample.drop(columns=['candidate_gold'], inplace=True)\ntwitter_sample.drop(columns=['relevant_yn_gold'], inplace=True)\ntwitter_sample.drop(columns=['retweet_count'], inplace=True)\ntwitter_sample.drop(columns=['sentiment_gold'], inplace=True)\ntwitter_sample.drop(columns=['subject_matter_gold'], inplace=True)\ntwitter_sample.drop(columns=['tweet_coord'], inplace=True)\ntwitter_sample.drop(columns=['tweet_created'], inplace=True)\ntwitter_sample.drop(columns=['tweet_id'], inplace=True)\ntwitter_sample.drop(columns=['user_timezone'], inplace=True)\ntwitter_sample.drop(columns=['name'], inplace=True)\ntwitter_sample.drop(columns=['tweet_location'], inplace=True)\n#twitter_sample = twitter_sample.drop(twitter_sample[twitter_sample.candidate != 'Donald Trump'].index)\ntwitter_sample = twitter_sample[twitter_sample.sentiment != \"Neutral\"]\ntwitter_sample = twitter_sample.head(10000)\nimport re\nimport string\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ntwitter_sample['text'] = twitter_sample['text'].apply(lambda x:clean_text(x))\n\nimport emoji\nimport spacy\nimport stop_words\ns = twitter_sample['text'].values\nlist_of_list = []\n#print(s)\ndef give_emoji_free_text(text):\n    return emoji.get_emoji_regexp().sub(r'', text)\n\nnlp = spacy.load(\"en_core_web_sm\")\nfor string in s:\n    string = string.lower()\n    string = give_emoji_free_text(string)\n    my_list = \"\"\n    doc = nlp(string)\n    #print(doc)\n   #print('1')\n    for token in doc:\n        if token.text[0:2] == \"rt\":\n            my_list == my_list\n        else:\n            lemm = token.lemma_\n            poss = token.pos_\n            stopp = token.is_stop\n            thisdict = {\n                \"lem\": lemm,\n                \"pos\": poss,\n                \"stop\": stopp\n            }\n            if (thisdict[\"stop\"] == False) & ((thisdict[\"pos\"] == 'PROPN') or (thisdict[\"pos\"] == 'VERB') or (thisdict[\"pos\"] == 'NOUN') or (thisdict[\"pos\"] == 'NUM') or (thisdict[\"pos\"] == 'ADV') or (thisdict[\"pos\"] == 'ADJ')):\n                my_list += thisdict[\"lem\"] + ' '\n            \n    list_of_list.append(my_list)\n\n#print(list_of_list.values)\ntwitter_sample['processing_text'] = list_of_list\n\ntwitter_sample.drop(columns=['id'], inplace=True)\ntwitter_sample.drop(columns=['candidate'], inplace=True)\ntwitter_sample.drop(columns=['candidate_confidence'], inplace=True)\ntwitter_sample.drop(columns=['sentiment_confidence'], inplace=True)\ntwitter_sample.drop(columns=['text'], inplace=True)\n\nlist_of_c = twitter_sample['processing_text']\nlist_of_r = []\n\nfor comment in list_of_c:\n    cplit = comment.split()\n    if len(cplit) < 3:\n        list_of_r.append('NaN')\n    elif len(cplit) > 27:\n        list_of_r.append(cplit[:27])\n    else:\n        list_of_r.append(comment)\n        \nprint(len(list_of_c))\nprint(len(list_of_r))\n\ntwitter_sample['correct_size'] = list_of_r\n\ntwitter_sample = twitter_sample[twitter_sample.correct_size != \"NaN\"]\ntwitter_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:38:40.807218Z","iopub.execute_input":"2021-08-09T00:38:40.807633Z","iopub.status.idle":"2021-08-09T00:40:32.625044Z","shell.execute_reply.started":"2021-08-09T00:38:40.8076Z","shell.execute_reply":"2021-08-09T00:40:32.623765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#twitter_sample['candidate'].unique()\ntwitter_sample.head(6)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:41:37.288316Z","iopub.execute_input":"2021-08-09T00:41:37.288784Z","iopub.status.idle":"2021-08-09T00:41:37.302913Z","shell.execute_reply.started":"2021-08-09T00:41:37.288747Z","shell.execute_reply":"2021-08-09T00:41:37.30161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For trump training","metadata":{}},{"cell_type":"code","source":"twitter_sample = twitter_sample.drop(twitter_sample[twitter_sample.candidate != 'Donald Trump'].index)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T16:06:46.904578Z","iopub.execute_input":"2021-07-09T16:06:46.904967Z","iopub.status.idle":"2021-07-09T16:06:46.919368Z","shell.execute_reply.started":"2021-07-09T16:06:46.904929Z","shell.execute_reply":"2021-07-09T16:06:46.918289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"for training and assigning sentiment","metadata":{}},{"cell_type":"code","source":"twitter_sample = twitter_sample.head(5000)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T17:10:21.411374Z","iopub.execute_input":"2021-07-09T17:10:21.41187Z","iopub.status.idle":"2021-07-09T17:10:21.416093Z","shell.execute_reply.started":"2021-07-09T17:10:21.411835Z","shell.execute_reply":"2021-07-09T17:10:21.414992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tensorflow model #1","metadata":{}},{"cell_type":"code","source":"youtube_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T16:17:45.621625Z","iopub.execute_input":"2021-07-13T16:17:45.622032Z","iopub.status.idle":"2021-07-13T16:17:45.637279Z","shell.execute_reply.started":"2021-07-13T16:17:45.621999Z","shell.execute_reply":"2021-07-13T16:17:45.636284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twitter_sample = twitter_sample.head(5000)\nyoutube_sample = youtube_sample.head(5000)\n\ntwitter_sample.info()\nreddit_sample.info()\nyoutube_sample.info()\ngeneric_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-13T16:18:19.745363Z","iopub.execute_input":"2021-07-13T16:18:19.745738Z","iopub.status.idle":"2021-07-13T16:18:19.788709Z","shell.execute_reply.started":"2021-07-13T16:18:19.745708Z","shell.execute_reply":"2021-07-13T16:18:19.787862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_sample.head(50)","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:26:31.372123Z","iopub.execute_input":"2021-07-13T17:26:31.372509Z","iopub.status.idle":"2021-07-13T17:26:31.389425Z","shell.execute_reply.started":"2021-07-13T17:26:31.372474Z","shell.execute_reply":"2021-07-13T17:26:31.388386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment = generic_sample['sentiment'].values\n#print(sentiment)\nneg = []\npos = []\n\nfor sent in sentiment:\n    if sent == 'Positive':\n        pos.append('pos')\n    elif sent == 'Negative':\n        neg.append('neg')\n        \nprint(len(neg))\nprint(len(pos))","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:28:24.537149Z","iopub.execute_input":"2021-07-13T17:28:24.537676Z","iopub.status.idle":"2021-07-13T17:28:24.547088Z","shell.execute_reply.started":"2021-07-13T17:28:24.537642Z","shell.execute_reply":"2021-07-13T17:28:24.545753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twitter_sample.info()\ndf.info()\nyoutube_comments.info()\n\ntwitter_sample = twitter_sample.head(6000)\ndf = df.head(6000)\n\nyoutube_comments = youtube_comments.head(6000)\nyoutube_comments.info()\ndf.info()\ntwitter_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:59:53.337177Z","iopub.execute_input":"2021-08-09T00:59:53.337539Z","iopub.status.idle":"2021-08-09T00:59:53.417302Z","shell.execute_reply.started":"2021-08-09T00:59:53.337509Z","shell.execute_reply":"2021-08-09T00:59:53.415925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twitter_sample.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:49:44.240841Z","iopub.execute_input":"2021-08-09T00:49:44.241336Z","iopub.status.idle":"2021-08-09T00:49:44.254106Z","shell.execute_reply.started":"2021-08-09T00:49:44.241303Z","shell.execute_reply":"2021-08-09T00:49:44.252311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"redditredo.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:13:20.858123Z","iopub.execute_input":"2021-08-09T05:13:20.858479Z","iopub.status.idle":"2021-08-09T05:13:20.875367Z","shell.execute_reply.started":"2021-08-09T05:13:20.85845Z","shell.execute_reply":"2021-08-09T05:13:20.873618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"youtube_comments.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T00:42:50.152741Z","iopub.execute_input":"2021-08-09T00:42:50.153132Z","iopub.status.idle":"2021-08-09T00:42:50.170266Z","shell.execute_reply.started":"2021-08-09T00:42:50.153101Z","shell.execute_reply":"2021-08-09T00:42:50.169531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generic_sentimentredo.info()\ngeneric_sentimentredo['sentiment'] = generic_sentimentredo['sentiment'].replace(1, 'positive')\ngeneric_sentimentredo['sentiment'] = generic_sentimentredo['sentiment'].replace(0, 'negative')\ngeneric_sentimentredo['sentiment'] = generic_sentimentredo['sentiment'].replace('Positive', 'negative')\ngeneric_sentimentredo['sentiment'] = generic_sentimentredo['sentiment'].replace('Negative', 'negative')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T01:37:36.537846Z","iopub.execute_input":"2021-08-09T01:37:36.538247Z","iopub.status.idle":"2021-08-09T01:37:36.564193Z","shell.execute_reply.started":"2021-08-09T01:37:36.538216Z","shell.execute_reply":"2021-08-09T01:37:36.562637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(generic_sentimentredo['comments'].values)\nX = tokenizer.texts_to_sequences(generic_sentimentredo['comments'].values)\nX = pad_sequences(X)\n\nembed_dim = 128\nlstm_out = 196\n\nmodelg = Sequential()\nmodelg.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodelg.add(SpatialDropout1D(0.4))\nmodelg.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodelg.add(Dense(2,activation='sigmoid'))\nmodelg.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(modelg.summary())\n\nY = pd.get_dummies(generic_sentimentredo['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)\n\nbatch_size = 32\nmodelg.fit(X_train, Y_train, epochs = 40, batch_size=batch_size, verbose = 2)\n\nvalidation_size = 800\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = modelg.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T01:37:38.243491Z","iopub.execute_input":"2021-08-09T01:37:38.24385Z","iopub.status.idle":"2021-08-09T01:46:13.296496Z","shell.execute_reply.started":"2021-08-09T01:37:38.24382Z","shell.execute_reply":"2021-08-09T01:46:13.294987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(youtube_comments['correct_length'].values)\nX = tokenizer.texts_to_sequences(youtube_comments['correct_length'].values)\nX = pad_sequences(X)\n\nembed_dim = 128\nlstm_out = 196\n\nmodely = Sequential()\nmodely.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodely.add(SpatialDropout1D(0.4))\nmodely.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodely.add(Dense(2,activation='sigmoid'))\nmodely.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(modely.summary())\n\nY = pd.get_dummies(youtube_comments['comp_score']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)\n\nbatch_size = 32\nmodely.fit(X_train, Y_train, epochs = 25, batch_size=batch_size, verbose = 2)\n\nvalidation_size = 800\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = modely.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T01:18:55.251338Z","iopub.execute_input":"2021-08-09T01:18:55.251767Z","iopub.status.idle":"2021-08-09T01:27:28.626992Z","shell.execute_reply.started":"2021-08-09T01:18:55.251734Z","shell.execute_reply":"2021-08-09T01:27:28.626022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(df['correct_size'].values)\nX = tokenizer.texts_to_sequences(df['correct_size'].values)\nX = pad_sequences(X)\n\nembed_dim = 128\nlstm_out = 196\n\nmodelr = Sequential()\nmodelr.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodelr.add(SpatialDropout1D(0.4))\nmodelr.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodelr.add(Dense(2,activation='sigmoid'))\nmodelr.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(modelr.summary())\n\nY = pd.get_dummies(df['comp_score']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)\n\nbatch_size = 32\nmodelr.fit(X_train, Y_train, epochs = 25, batch_size=batch_size, verbose = 2)\n\nvalidation_size = 800\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = modelr.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:26:21.52042Z","iopub.execute_input":"2021-08-09T02:26:21.520774Z","iopub.status.idle":"2021-08-09T02:31:43.664656Z","shell.execute_reply.started":"2021-08-09T02:26:21.520744Z","shell.execute_reply":"2021-08-09T02:31:43.663287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"texter = \"trump bad really dumb\"\n\nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(twitter_sample['correct_size'].values)\nX = tokenizer.texts_to_sequences(twitter_sample['correct_size'].values)\nX = pad_sequences(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\n\nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(twitter_sample['correct_size'].values)\nX = tokenizer.texts_to_sequences(twitter_sample['correct_size'].values)\nX = pad_sequences(X)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:19:26.891257Z","iopub.execute_input":"2021-08-09T02:19:26.89167Z","iopub.status.idle":"2021-08-09T02:19:27.170811Z","shell.execute_reply.started":"2021-08-09T02:19:26.891624Z","shell.execute_reply":"2021-08-09T02:19:27.169689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\nmodel.add(SpatialDropout1D(0.4))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:19:28.457595Z","iopub.execute_input":"2021-08-09T02:19:28.458041Z","iopub.status.idle":"2021-08-09T02:19:28.652142Z","shell.execute_reply.started":"2021-08-09T02:19:28.458001Z","shell.execute_reply":"2021-08-09T02:19:28.650847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = pd.get_dummies(twitter_sample['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:19:30.795383Z","iopub.execute_input":"2021-08-09T02:19:30.795906Z","iopub.status.idle":"2021-08-09T02:19:30.806498Z","shell.execute_reply.started":"2021-08-09T02:19:30.795859Z","shell.execute_reply":"2021-08-09T02:19:30.805556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nmodel.fit(X_train, Y_train, epochs = 25, batch_size=batch_size, verbose = 2)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:19:32.672962Z","iopub.execute_input":"2021-08-09T02:19:32.67351Z","iopub.status.idle":"2021-08-09T02:23:14.610536Z","shell.execute_reply.started":"2021-08-09T02:19:32.673455Z","shell.execute_reply":"2021-08-09T02:23:14.609651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_size = 800\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:25:08.683068Z","iopub.execute_input":"2021-08-09T02:25:08.683735Z","iopub.status.idle":"2021-08-09T02:25:09.497233Z","shell.execute_reply.started":"2021-08-09T02:25:08.683697Z","shell.execute_reply":"2021-08-09T02:25:09.496367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(X_validate)):\n    \n    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(Y_validate[x]):\n        if np.argmax(Y_validate[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(Y_validate[x]) == 0:\n        neg_cnt += 1\n    else:\n        pos_cnt += 1\n\n\n\nprint(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")","metadata":{"execution":{"iopub.status.busy":"2021-07-13T17:56:54.475134Z","iopub.execute_input":"2021-07-13T17:56:54.475498Z","iopub.status.idle":"2021-07-13T17:57:28.494361Z","shell.execute_reply.started":"2021-07-13T17:56:54.475468Z","shell.execute_reply":"2021-07-13T17:57:28.493262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twt = \"@realDonaldTrump delivered the highest ratings in the history of presidential debates. #Trump2016\"\n\ndef give_emoji_free_text(text):\n    return emoji.get_emoji_regexp().sub(r'', text)\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ntwt = twt.lower()\ntwt = give_emoji_free_text(twt)\nmy_list = \"\"\ndoc = nlp(twt)\n    #print(doc)\n   #print('1')\nfor token in doc:\n    if token.text[0:2] == \"rt\":\n        my_list == my_list\n    else:\n        lemm = token.lemma_\n        poss = token.pos_\n        stopp = token.is_stop\n        thisdict = {\n            \"lem\": lemm,\n            \"pos\": poss,\n            \"stop\": stopp\n        }\n        if (thisdict[\"stop\"] == False) & ((thisdict[\"pos\"] == 'PROPN') or (thisdict[\"pos\"] == 'VERB') or (thisdict[\"pos\"] == 'NOUN') or (thisdict[\"pos\"] == 'NUM') or (thisdict[\"pos\"] == 'ADV') or (thisdict[\"pos\"] == 'ADJ')):\n            my_list += thisdict[\"lem\"] + ' '\n            \ntwt = my_list\n\n#vectorizing the tweet by the pre-fitted tokenizer instance\ntwt = tokenizer.texts_to_sequences(twt)\n#padding the tweet to have exactly the same shape as `embedding_2` input\ntwt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\nprint(twt)\nsentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\nif(np.argmax(sentiment) == 0):\n    print(\"negative\")\nelif (np.argmax(sentiment) == 1):\n    print(\"positive\")","metadata":{"execution":{"iopub.status.busy":"2021-08-09T03:20:18.325563Z","iopub.execute_input":"2021-08-09T03:20:18.325955Z","iopub.status.idle":"2021-08-09T03:20:19.784201Z","shell.execute_reply.started":"2021-08-09T03:20:18.325921Z","shell.execute_reply":"2021-08-09T03:20:19.782804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T14:13:08.262751Z","iopub.execute_input":"2021-07-12T14:13:08.263094Z","iopub.status.idle":"2021-07-12T14:13:08.277526Z","shell.execute_reply.started":"2021-07-12T14:13:08.263059Z","shell.execute_reply":"2021-07-12T14:13:08.276749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_sentiment['comments'] = list_of_reddit_comments","metadata":{"execution":{"iopub.status.busy":"2021-07-12T14:15:59.170385Z","iopub.execute_input":"2021-07-12T14:15:59.170754Z","iopub.status.idle":"2021-07-12T14:15:59.175956Z","shell.execute_reply.started":"2021-07-12T14:15:59.170723Z","shell.execute_reply":"2021-07-12T14:15:59.175014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(list_of_reddit_comments))","metadata":{"execution":{"iopub.status.busy":"2021-07-12T14:16:25.542483Z","iopub.execute_input":"2021-07-12T14:16:25.542843Z","iopub.status.idle":"2021-07-12T14:16:25.546777Z","shell.execute_reply.started":"2021-07-12T14:16:25.542815Z","shell.execute_reply":"2021-07-12T14:16:25.546097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twitter_sample = twitter_sample.head(500)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"youtube_sample['correct_length']\n\nlist_of_twitter_comments = youtube_sample['correct_length'].values\nprint(len(list_of_twitter_comments))\nlist_of_sentiment = []\n\nfor comment in list_of_twitter_comments:\n    #vectorizing the tweet by the pre-fitted tokenizer instance\n    twt = comment\n    twt = tokenizer.texts_to_sequences(twt)\n    #padding the tweet to have exactly the same shape as `embedding_2` input\n    twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n    #print(twt)\n    sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n    if(np.argmax(sentiment) == 0):\n        list_of_sentiment.append(\"negative\")\n    elif (np.argmax(sentiment) == 1):\n        list_of_sentiment.append(\"positive\")\n        \nprint(len(list_of_sentiment))\nyoutube_sentiment = pd.DataFrame(data=list_of_twitter_comments, columns = ['comments'])\nyoutube_sentiment['sentiment'] = list_of_sentiment\nyoutube_sentiment['comments'] = list_of_twitter_comments","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:55:41.973639Z","iopub.execute_input":"2021-07-12T18:55:41.973962Z","iopub.status.idle":"2021-07-12T19:26:20.588568Z","shell.execute_reply.started":"2021-07-12T18:55:41.973934Z","shell.execute_reply":"2021-07-12T19:26:20.587737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"youtube_sentiment.to_csv('youtube_sentiment.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T19:28:29.360152Z","iopub.execute_input":"2021-07-12T19:28:29.360818Z","iopub.status.idle":"2021-07-12T19:28:29.406491Z","shell.execute_reply.started":"2021-07-12T19:28:29.360772Z","shell.execute_reply":"2021-07-12T19:28:29.405537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_reddit_comments = df['correct_size'].values\nprint(len(list_of_reddit_comments))\nlist_of_sentiment = []\n\nfor comment in list_of_reddit_comments:\n    #vectorizing the tweet by the pre-fitted tokenizer instance\n    twt = comment\n    twt = tokenizer.texts_to_sequences(twt)\n    #padding the tweet to have exactly the same shape as `embedding_2` input\n    twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n    #print(twt)\n    sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n    if(np.argmax(sentiment) == 0):\n        list_of_sentiment.append(\"negative\")\n    elif (np.argmax(sentiment) == 1):\n        list_of_sentiment.append(\"positive\")\n        \nprint(len(list_of_sentiment))\nreddit_sentiment = pd.DataFrame(data=list_of_reddit_comments, columns = ['comments'])\nreddit_sentiment['sentiment'] = list_of_sentiment\nreddit_sentiment['comments'] = list_of_reddit_comments","metadata":{"execution":{"iopub.status.busy":"2021-07-12T13:38:03.882261Z","iopub.execute_input":"2021-07-12T13:38:03.882603Z","iopub.status.idle":"2021-07-12T14:10:12.157541Z","shell.execute_reply.started":"2021-07-12T13:38:03.882576Z","shell.execute_reply":"2021-07-12T14:10:12.156885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_sentiment = pd.DataFrame(data=list_of_reddit_comments, columns = ['comments'])\nreddit_sentiment['sentiment'] = list_of_sentiment\nreddit_sentiment['comments'] = df['correct_size']","metadata":{"execution":{"iopub.status.busy":"2021-07-12T12:54:01.074642Z","iopub.execute_input":"2021-07-12T12:54:01.074987Z","iopub.status.idle":"2021-07-12T12:54:01.085464Z","shell.execute_reply.started":"2021-07-12T12:54:01.074957Z","shell.execute_reply":"2021-07-12T12:54:01.084451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_sentiment.dropna(inplace=True)\nreddit_sentiment.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T14:20:58.118054Z","iopub.execute_input":"2021-07-12T14:20:58.118574Z","iopub.status.idle":"2021-07-12T14:20:58.150512Z","shell.execute_reply.started":"2021-07-12T14:20:58.118535Z","shell.execute_reply":"2021-07-12T14:20:58.149696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_sentiment.to_csv('reddit_sentiment.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T14:25:40.264651Z","iopub.execute_input":"2021-07-12T14:25:40.264977Z","iopub.status.idle":"2021-07-12T14:25:40.298914Z","shell.execute_reply.started":"2021-07-12T14:25:40.26495Z","shell.execute_reply":"2021-07-12T14:25:40.29829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_sentiment['comments'] = df['correct_size']\nreddit_sentiment['sentiment'] = list_of_sentiment","metadata":{"execution":{"iopub.status.busy":"2021-07-12T12:52:30.767224Z","iopub.execute_input":"2021-07-12T12:52:30.767568Z","iopub.status.idle":"2021-07-12T12:52:30.77349Z","shell.execute_reply.started":"2021-07-12T12:52:30.76754Z","shell.execute_reply":"2021-07-12T12:52:30.772332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_sentiment.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-12T14:12:00.704911Z","iopub.execute_input":"2021-07-12T14:12:00.705437Z","iopub.status.idle":"2021-07-12T14:12:00.718898Z","shell.execute_reply.started":"2021-07-12T14:12:00.705384Z","shell.execute_reply":"2021-07-12T14:12:00.718226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_sample = pd.read_csv(\"/kaggle/input/twitter/Sentiment.csv\")\n\ntesting_sample.drop(columns=['relevant_yn'], inplace=True)\ntesting_sample.drop(columns=['relevant_yn_confidence'], inplace=True)\ntesting_sample.drop(columns=['subject_matter'], inplace=True)\ntesting_sample.drop(columns=['subject_matter_confidence'], inplace=True)\ntesting_sample.drop(columns=['candidate_gold'], inplace=True)\ntesting_sample.drop(columns=['relevant_yn_gold'], inplace=True)\ntesting_sample.drop(columns=['retweet_count'], inplace=True)\ntesting_sample.drop(columns=['sentiment_gold'], inplace=True)\ntesting_sample.drop(columns=['subject_matter_gold'], inplace=True)\ntesting_sample.drop(columns=['tweet_coord'], inplace=True)\ntesting_sample.drop(columns=['tweet_created'], inplace=True)\ntesting_sample.drop(columns=['tweet_id'], inplace=True)\ntesting_sample.drop(columns=['user_timezone'], inplace=True)\ntesting_sample.drop(columns=['name'], inplace=True)\ntesting_sample.drop(columns=['tweet_location'], inplace=True)\ntesting_sample = testing_sample.drop(testing_sample[testing_sample.candidate != 'Donald Trump'].index)\ntesting_sample = testing_sample[testing_sample.sentiment != \"Neutral\"]\nimport re\nimport string\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ntesting_sample['text'] = testing_sample['text'].apply(lambda x:clean_text(x))\n\nimport emoji\nimport spacy\nimport stop_words\ns = testing_sample['text'].values\nlist_of_list = []\n#print(s)\ndef give_emoji_free_text(text):\n    return emoji.get_emoji_regexp().sub(r'', text)\n\nnlp = spacy.load(\"en_core_web_sm\")\nfor string in s:\n    string = string.lower()\n    string = give_emoji_free_text(string)\n    my_list = \"\"\n    doc = nlp(string)\n    #print(doc)\n   #print('1')\n    for token in doc:\n        if token.text[0:2] == \"rt\":\n            my_list == my_list\n        else:\n            lemm = token.lemma_\n            poss = token.pos_\n            stopp = token.is_stop\n            thisdict = {\n                \"lem\": lemm,\n                \"pos\": poss,\n                \"stop\": stopp\n            }\n            if (thisdict[\"stop\"] == False) & ((thisdict[\"pos\"] == 'PROPN') or (thisdict[\"pos\"] == 'VERB') or (thisdict[\"pos\"] == 'NOUN') or (thisdict[\"pos\"] == 'NUM') or (thisdict[\"pos\"] == 'ADV') or (thisdict[\"pos\"] == 'ADJ')):\n                my_list += thisdict[\"lem\"] + ' '\n            \n    list_of_list.append(my_list)\n\n#print(list_of_list.values)\ntesting_sample['processing_text'] = list_of_list\n\ntesting_sample.drop(columns=['id'], inplace=True)\ntesting_sample.drop(columns=['candidate'], inplace=True)\ntesting_sample.drop(columns=['candidate_confidence'], inplace=True)\ntesting_sample.drop(columns=['sentiment_confidence'], inplace=True)\ntesting_sample.drop(columns=['text'], inplace=True)\ntesting_sample.head(10)\ntesting_sample.info","metadata":{"execution":{"iopub.status.busy":"2021-07-09T17:40:46.826054Z","iopub.execute_input":"2021-07-09T17:40:46.826596Z","iopub.status.idle":"2021-07-09T17:41:13.299973Z","shell.execute_reply.started":"2021-07-09T17:40:46.826549Z","shell.execute_reply":"2021-07-09T17:41:13.29889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_sample = testing_sample.head(500)\ntesting_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-09T17:44:30.939841Z","iopub.execute_input":"2021-07-09T17:44:30.940362Z","iopub.status.idle":"2021-07-09T17:44:30.953728Z","shell.execute_reply.started":"2021-07-09T17:44:30.940328Z","shell.execute_reply":"2021-07-09T17:44:30.952582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_result = []\ngold_result = testing_sample['sentiment']\ntesting_set = np.asarray(testing_sample['processing_text'])\n\n\nfor opinion in testing_set:\n    opinion = tokenizer.texts_to_sequences(opinion)\n    #padding the tweet to have exactly the same shape as `embedding_2` input\n    opinion = pad_sequences(opinion, maxlen=28, dtype='int32', value=0)\n    #print(twt)\n    sentiment = model.predict(opinion,batch_size=1,verbose = 2)[0]\n    if(np.argmax(sentiment) == 0):    \n         test_result.append(\"negative\")\n    elif (np.argmax(sentiment) == 1):       \n        test_result.append(\"positive\")\n    \n    \n    #print(opinion)","metadata":{"execution":{"iopub.status.busy":"2021-07-09T17:59:55.913026Z","iopub.execute_input":"2021-07-09T17:59:55.91344Z","iopub.status.idle":"2021-07-09T18:04:01.547743Z","shell.execute_reply.started":"2021-07-09T17:59:55.913408Z","shell.execute_reply":"2021-07-09T18:04:01.546577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(gold_result))","metadata":{"execution":{"iopub.status.busy":"2021-07-09T18:21:04.891606Z","iopub.execute_input":"2021-07-09T18:21:04.892102Z","iopub.status.idle":"2021-07-09T18:21:04.897289Z","shell.execute_reply.started":"2021-07-09T18:21:04.892069Z","shell.execute_reply":"2021-07-09T18:21:04.896199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(testing_set)):\n    #vectorizing the tweet by the pre-fitted tokenizer instance\n    twt = tokenizer.texts_to_sequences(twt)\n    #padding the tweet to have exactly the same shape as `embedding_2` input\n    twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n    #print(twt)\n    sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n    if(np.argmax(sentiment) == 0):\n        print(\"negative\")\n    elif (np.argmax(sentiment) == 1):\n        print(\"positive\")\n    test_result.append(classifier.classify(testing_set[i][0]))\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pos_count = 0\nneg_count = 0\ntwitter = testingdf['sentiment']\nfor sentiment in twitter:\n    if sentiment == 'positive':\n        pos_count = pos_count + 1\n    elif sentiment == 'negative':\n        neg_count = neg_count + 1\n        \nprint(neg_count)\nprint(pos_count)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:52:18.880665Z","iopub.execute_input":"2021-08-09T05:52:18.881034Z","iopub.status.idle":"2021-08-09T05:52:18.887377Z","shell.execute_reply.started":"2021-08-09T05:52:18.881003Z","shell.execute_reply":"2021-08-09T05:52:18.886551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_df = pd.DataFrame(data=predictions, columns = ['preditcions'])","metadata":{"execution":{"iopub.status.busy":"2021-07-14T16:44:48.18968Z","iopub.execute_input":"2021-07-14T16:44:48.190138Z","iopub.status.idle":"2021-07-14T16:44:48.199007Z","shell.execute_reply.started":"2021-07-14T16:44:48.190098Z","shell.execute_reply":"2021-07-14T16:44:48.19784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twitter = youtube_sample['sentiment']","metadata":{"execution":{"iopub.status.busy":"2021-07-14T17:06:51.427685Z","iopub.execute_input":"2021-07-14T17:06:51.428109Z","iopub.status.idle":"2021-07-14T17:06:51.432787Z","shell.execute_reply.started":"2021-07-14T17:06:51.428076Z","shell.execute_reply":"2021-07-14T17:06:51.431751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing_df['gold_pred'] = list_of_g_s\ntesting_df['generic_pred'] = predictions_g","metadata":{"execution":{"iopub.status.busy":"2021-07-14T16:54:08.144231Z","iopub.execute_input":"2021-07-14T16:54:08.144805Z","iopub.status.idle":"2021-07-14T16:54:08.150447Z","shell.execute_reply.started":"2021-07-14T16:54:08.144747Z","shell.execute_reply":"2021-07-14T16:54:08.149206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testingdf['sm_predictions_traditional'] = predictions\ntestingdf['generic_predictions_traditional'] = predictions_g","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:12:06.181249Z","iopub.execute_input":"2021-08-09T04:12:06.18161Z","iopub.status.idle":"2021-08-09T04:12:06.216522Z","shell.execute_reply.started":"2021-08-09T04:12:06.18158Z","shell.execute_reply":"2021-08-09T04:12:06.215179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testingdf.head(300)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:03:05.42179Z","iopub.execute_input":"2021-08-09T05:03:05.422338Z","iopub.status.idle":"2021-08-09T05:03:05.439521Z","shell.execute_reply.started":"2021-08-09T05:03:05.422288Z","shell.execute_reply":"2021-08-09T05:03:05.438426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re, string\nlistdonetwo = []\ntesting = testingdf['generic_predictions_traditional']\nprint(type(testing[1]))\nstring = str(testing[1])\nprint((string))\nfor s in testing:\n    strs = str(s)\n    s = re.sub(r'[^\\w\\s]','',strs)\n    listdonetwo.append(s)\n\n    \nprint(len(listdonetwo))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:00:36.266567Z","iopub.execute_input":"2021-08-09T05:00:36.267096Z","iopub.status.idle":"2021-08-09T05:00:36.29124Z","shell.execute_reply.started":"2021-08-09T05:00:36.267062Z","shell.execute_reply":"2021-08-09T05:00:36.290473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testingdf['sentiment'] = testingdf['sentiment'].replace('Positive', 'positive')\ntestingdf['sentiment'] = testingdf['sentiment'].replace('Negative', 'negative')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:01:19.482916Z","iopub.execute_input":"2021-08-09T05:01:19.483286Z","iopub.status.idle":"2021-08-09T05:01:19.489948Z","shell.execute_reply.started":"2021-08-09T05:01:19.483255Z","shell.execute_reply":"2021-08-09T05:01:19.489097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testingdf['generic_predictions_traditional'] = listdonetwo","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:00:47.510518Z","iopub.execute_input":"2021-08-09T05:00:47.510883Z","iopub.status.idle":"2021-08-09T05:00:47.516979Z","shell.execute_reply.started":"2021-08-09T05:00:47.510852Z","shell.execute_reply":"2021-08-09T05:00:47.515133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testingdf['sm_predictions_traditional'] = listdone","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:59:51.973458Z","iopub.execute_input":"2021-08-09T04:59:51.974022Z","iopub.status.idle":"2021-08-09T04:59:51.978644Z","shell.execute_reply.started":"2021-08-09T04:59:51.973971Z","shell.execute_reply":"2021-08-09T04:59:51.977972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testing_df['sm_predictions_traditional'] = testing_df['sm_predictions_traditional'].replace('[positive]', 'positive')\ntesting_df['sm_predictions_traditional'] = testing_df['sm_predictions_traditional'].replace('[negative]', 'negative')\ntesting_df['preditcions'] = testing_df['preditcions'].replace('Positive', 'negative')\ntesting_df['preditcions'] = testing_df['preditcions'].replace('Negative', 'negative')\n\ntesting_df['gold_pred'] = testing_df['gold_pred'].replace(1, 'positive')\ntesting_df['gold_pred'] = testing_df['gold_pred'].replace(0, 'negative')\ntesting_df['gold_pred'] = testing_df['gold_pred'].replace('Positive', 'negative')\ntesting_df['gold_pred'] = testing_df['gold_pred'].replace('Negative', 'negative')\n\ntesting_df['generic_pred'] = testing_df['generic_pred'].replace(1, 'positive')\ntesting_df['generic_pred'] = testing_df['generic_pred'].replace(0, 'negative')\ntesting_df['generic_pred'] = testing_df['generic_pred'].replace('Positive', 'negative')\ntesting_df['generic_pred'] = testing_df['generic_pred'].replace('Negative', 'negative')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T16:54:30.059159Z","iopub.execute_input":"2021-07-14T16:54:30.059552Z","iopub.status.idle":"2021-07-14T16:54:30.07697Z","shell.execute_reply.started":"2021-07-14T16:54:30.05952Z","shell.execute_reply":"2021-07-14T16:54:30.075743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix   \n\ntest_result = testingdf['generic_predictions_traditional'].values\ngold_result = testingdf['sentiment'].values\n\n#print('\\nClasification report:\\n', classification_report(gold_result, test_result))\nprint('\\nConfussion matrix:\\n',confusion_matrix(gold_result, test_result))    ","metadata":{"execution":{"iopub.status.busy":"2021-08-09T08:00:47.39231Z","iopub.execute_input":"2021-08-09T08:00:47.392741Z","iopub.status.idle":"2021-08-09T08:00:47.415695Z","shell.execute_reply.started":"2021-08-09T08:00:47.392705Z","shell.execute_reply":"2021-08-09T08:00:47.414326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import plot_confusion_matrix\nimport matplotlib.pyplot as plt  \nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nclf = SVC(random_state=0)\nclf.fit(X_train, y_train)\nSVC(random_state=0)\nplot_confusion_matrix(clf, gold_result, test_result)  \nplt.show()  \n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T08:00:43.331369Z","iopub.execute_input":"2021-08-09T08:00:43.3318Z","iopub.status.idle":"2021-08-09T08:00:44.611879Z","shell.execute_reply.started":"2021-08-09T08:00:43.331764Z","shell.execute_reply":"2021-08-09T08:00:44.60902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http://matplotlib.org/examples/color/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T08:03:35.777716Z","iopub.execute_input":"2021-08-09T08:03:35.778131Z","iopub.status.idle":"2021-08-09T08:03:35.794651Z","shell.execute_reply.started":"2021-08-09T08:03:35.778098Z","shell.execute_reply":"2021-08-09T08:03:35.793053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"plot_confusion_matrix( np.array([[   167,   6],\n                                              [    116,  211]]), \n                      normalize    = False,\n                      target_names = ['Positive', 'Negative'],\n                      title        = \"Confusion Matrix, Deep learning combined\")","metadata":{"execution":{"iopub.status.busy":"2021-08-09T06:07:55.355265Z","iopub.execute_input":"2021-08-09T06:07:55.355667Z","iopub.status.idle":"2021-08-09T06:07:55.642042Z","shell.execute_reply.started":"2021-08-09T06:07:55.355625Z","shell.execute_reply":"2021-08-09T06:07:55.640817Z"}}},{"cell_type":"code","source":"plot_confusion_matrix(cm = np.array([[   167,   6],\n                                              [    116,  211]]), \n                      normalize    = False,\n                      target_names = ['Positive', 'Negative'],\n                      title        = \"Confusion Matrix, Deep learning generic\")","metadata":{"execution":{"iopub.status.busy":"2021-08-09T08:03:40.034036Z","iopub.execute_input":"2021-08-09T08:03:40.034424Z","iopub.status.idle":"2021-08-09T08:03:40.376481Z","shell.execute_reply.started":"2021-08-09T08:03:40.03439Z","shell.execute_reply":"2021-08-09T08:03:40.375364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-08-09T08:00:52.159076Z","iopub.execute_input":"2021-08-09T08:00:52.15968Z","iopub.status.idle":"2021-08-09T08:00:52.184345Z","shell.execute_reply.started":"2021-08-09T08:00:52.159645Z","shell.execute_reply":"2021-08-09T08:00:52.183009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CM = nltk.ConfusionMatrix(gold_result, test_result)\nprint(CM)\n\nprint(\"Naive Bayes Algo accuracy percent:\"+str((nltk.classify.accuracy(classifier, testing_set))*100)+\"\\n\")\n\nlabels = {'pos', 'neg'}\n\nfrom collections import Counter\nTP, FN, FP = Counter(), Counter(), Counter()\nfor i in labels:\n    for j in labels:\n        if i == j:\n            TP[i] += int(CM[i,j])\n        else:\n            FN[i] += int(CM[i,j])\n            FP[j] += int(CM[i,j])\n\nprint(\"label\\tprecision\\trecall\\tf_measure\")\nfor label in sorted(labels):\n    precision, recall = 0, 0\n    if TP[label] == 0:\n        f_measure = 0\n    else:\n        precision = float(TP[label]) / (TP[label]+FP[label])\n        recall = float(TP[label]) / (TP[label]+FN[label])\n        f_measure = float(2) * (precision * recall) / (precision + recall)\n    print(label+\"\\t\"+str(precision)+\"\\t\"+str(recall)+\"\\t\"+str(f_measure))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing phase","metadata":{}},{"cell_type":"code","source":"twitter_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T15:17:17.149145Z","iopub.execute_input":"2021-07-14T15:17:17.149539Z","iopub.status.idle":"2021-07-14T15:17:17.16622Z","shell.execute_reply.started":"2021-07-14T15:17:17.149507Z","shell.execute_reply":"2021-07-14T15:17:17.164843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(generic_sample['comments'],generic_sample['sentiment'],test_size = 0.2 , random_state = 0)\n\ntvg=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n#transformed train reviews\ntv_train_reviews=tvg.fit_transform(x_train)\n#transformed test reviews\ntv_test_reviews=tvg.transform(x_test)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)\n\n#training the model\nmnbg=MultinomialNB()\n\n#fitting the nb for tfidf features\nmnb_tfidf=mnbg.fit(tv_train_reviews,y_train)\nprint(mnb_tfidf)\n\n#pred_list = []\nprediction = \"Trump sucks ass\"\nprediction = [prediction]\nprediction=tvg.transform(prediction)\n#prediction = str(prediction)\n#pred_list.append(prediction)\n#prediction = [prediction]\nmnb_tfidf_predict = mnbg.predict(prediction)\nprint(mnb_tfidf_predict)\n\nx_train,x_test,y_train,y_test = train_test_split(twitter_sample['processing_text'],twitter_sample['sentiment'],test_size = 0.2 , random_state = 0)\n\ntvt=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n#transformed train reviews\ntv_train_reviews=tvt.fit_transform(x_train)\n#transformed test reviews\ntv_test_reviews=tvt.transform(x_test)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)\n\n#training the model\nmnbt=MultinomialNB()\n\n#fitting the nb for tfidf features\nmnb_tfidf=mnbt.fit(tv_train_reviews,y_train)\nprint(mnb_tfidf)\n\n#pred_list = []\nprediction = \"Trump sucks ass\"\nprediction = [prediction]\nprediction=tvt.transform(prediction)\n#prediction = str(prediction)\n#pred_list.append(prediction)\n#prediction = [prediction]\nmnb_tfidf_predict = mnbt.predict(prediction)\nprint(mnb_tfidf_predict)\n\nx_train,x_test,y_train,y_test = train_test_split(reddit_sample['comments'],reddit_sample['sentiment'],test_size = 0.2 , random_state = 0)\n\ntvr=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n#transformed train reviews\ntv_train_reviews=tvr.fit_transform(x_train)\n#transformed test reviews\ntv_test_reviews=tvr.transform(x_test)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)\n\n#training the model\nmnbr=MultinomialNB()\n\n#fitting the nb for tfidf features\nmnb_tfidf=mnbr.fit(tv_train_reviews,y_train)\nprint(mnb_tfidf)\n\n#pred_list = []\nprediction = \"Trump sucks ass\"\nprediction = [prediction]\nprediction=tvr.transform(prediction)\n#prediction = str(prediction)\n#pred_list.append(prediction)\n#prediction = [prediction]\nmnb_tfidf_predict = mnbr.predict(prediction)\nprint(mnb_tfidf_predict)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T15:22:08.957322Z","iopub.execute_input":"2021-07-14T15:22:08.957692Z","iopub.status.idle":"2021-07-14T15:22:10.378171Z","shell.execute_reply.started":"2021-07-14T15:22:08.95766Z","shell.execute_reply":"2021-07-14T15:22:10.377097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Building the prediction set","metadata":{}},{"cell_type":"code","source":"twitter_sample.info()\nreddit_sample.info()\nyoutube_sample.info()\ngeneric_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-07-14T15:53:37.93786Z","iopub.execute_input":"2021-07-14T15:53:37.938229Z","iopub.status.idle":"2021-07-14T15:53:37.976305Z","shell.execute_reply.started":"2021-07-14T15:53:37.938196Z","shell.execute_reply":"2021-07-14T15:53:37.974926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T03:26:30.266861Z","iopub.execute_input":"2021-08-09T03:26:30.267278Z","iopub.status.idle":"2021-08-09T03:26:30.278842Z","shell.execute_reply.started":"2021-08-09T03:26:30.267244Z","shell.execute_reply":"2021-08-09T03:26:30.277855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#redditredo.head()\n#youtube_comments.head()\n#twitter_sample.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T03:44:52.351851Z","iopub.execute_input":"2021-08-09T03:44:52.35226Z","iopub.status.idle":"2021-08-09T03:44:52.370004Z","shell.execute_reply.started":"2021-08-09T03:44:52.352226Z","shell.execute_reply":"2021-08-09T03:44:52.368908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"redditredo.info()\nyoutube_comments.info()\ntwitter_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T03:29:54.270843Z","iopub.execute_input":"2021-08-09T03:29:54.271232Z","iopub.status.idle":"2021-08-09T03:29:54.312746Z","shell.execute_reply.started":"2021-08-09T03:29:54.2712Z","shell.execute_reply":"2021-08-09T03:29:54.311382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twitter_test = twitter_sample.head(166)\nyoutube_test = youtube_comments.head(167)\nreddit_test = redditredo.head(167)\nprediction_set = []\n\nlist_of_t_c = twitter_test['correct_size'].values\nlist_of_t_s = twitter_test['sentiment'].values\nlist_of_r_c = reddit_test['comments'].values\nlist_of_r_s = reddit_test['comp_score'].values\nlist_of_y_c = youtube_test['text'].values\nlist_of_y_s = youtube_test['comp_score'].values\nlist_of_g_c = []\nlist_of_g_s = []\n\nfor thing in list_of_t_c:\n    list_of_g_c.append(thing)\nfor thing in list_of_r_c:\n    list_of_g_c.append(thing)\nfor thing in list_of_y_c:\n    list_of_g_c.append(thing)\n\nfor thing in list_of_t_s:\n    list_of_g_s.append(thing)\nfor thing in list_of_r_s:\n    list_of_g_s.append(thing)\nfor thing in list_of_y_s:\n    list_of_g_s.append(thing)    \n\nprint(len(list_of_g_c))\nprint(len(list_of_g_s))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T03:31:41.415298Z","iopub.execute_input":"2021-08-09T03:31:41.415972Z","iopub.status.idle":"2021-08-09T03:31:41.430085Z","shell.execute_reply.started":"2021-08-09T03:31:41.41586Z","shell.execute_reply":"2021-08-09T03:31:41.428955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testingdf = pd.DataFrame(data=list_of_g_c, columns = ['comments'])\n#testingdf = testingdf[testingdf.comments != \"NaN\"]\ntestingdf['sentiment'] = list_of_g_s\ntestingdf.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T03:34:18.252299Z","iopub.execute_input":"2021-08-09T03:34:18.252862Z","iopub.status.idle":"2021-08-09T03:34:18.270507Z","shell.execute_reply.started":"2021-08-09T03:34:18.252812Z","shell.execute_reply":"2021-08-09T03:34:18.268987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twitter_sample['sentiment'] = twitter_sample['sentiment'].replace('Positive', 'positive')\ntwitter_sample['sentiment'] = twitter_sample['sentiment'].replace('Negative', 'negative')","metadata":{"execution":{"iopub.status.busy":"2021-08-09T03:45:38.309565Z","iopub.execute_input":"2021-08-09T03:45:38.309923Z","iopub.status.idle":"2021-08-09T03:45:38.316366Z","shell.execute_reply.started":"2021-08-09T03:45:38.309879Z","shell.execute_reply":"2021-08-09T03:45:38.315564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testingdf.head()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T03:45:49.030779Z","iopub.execute_input":"2021-08-09T03:45:49.031184Z","iopub.status.idle":"2021-08-09T03:45:49.044844Z","shell.execute_reply.started":"2021-08-09T03:45:49.031153Z","shell.execute_reply":"2021-08-09T03:45:49.043355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" twt = comment\n        twt = tokenizer.texts_to_sequences(twt)\n        #padding the tweet to have exactly the same shape as `embedding_2` input\n        twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n        #print(twt)\n        sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n        if(np.argmax(sentiment) == 0):\n            predictions.append(\"negative\")\n        elif (np.argmax(sentiment) == 1):\n            predictions.append(\"positive\")\n            \n            max_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(twitter_sample['correct_size'].values)\nX = tokenizer.texts_to_sequences(twitter_sample['correct_size'].values)\nX = pad_sequences(X)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"redditredo.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:34:19.286423Z","iopub.execute_input":"2021-08-09T04:34:19.286954Z","iopub.status.idle":"2021-08-09T04:34:19.306518Z","shell.execute_reply.started":"2021-08-09T04:34:19.286913Z","shell.execute_reply":"2021-08-09T04:34:19.3054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_g_c = testingdf['comments'].values\nlist_of_g_c = list_of_g_c.tolist()\nmax_fatures = 2000\n\npredictions_dl = []\nfor comment in list_of_g_c:\n    index = list_of_g_c.index(comment)\n    if index >= 0 & index < 166:\n        twt = comment\n        tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n        tokenizer.fit_on_texts(twitter_sample['correct_size'].values)\n        twt = tokenizer.texts_to_sequences(twt)\n        #padding the tweet to have exactly the same shape as `embedding_2` input\n        twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n        #print(twt)\n        sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n        if(np.argmax(sentiment) == 0):\n            predictions_dl.append(\"negative\")\n        elif (np.argmax(sentiment) == 1):\n            predictions_dl.append(\"positive\")\n    elif index >= 166 & index < 333:\n        twt = comment\n        tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n        tokenizer.fit_on_texts(redditredo['comments'].values)\n       \n        twt = tokenizer.texts_to_sequences(twt)\n        #padding the tweet to have exactly the same shape as `embedding_2` input\n        twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n        #print(twt)\n        sentiment = modelr.predict(twt,batch_size=1,verbose = 2)[0]\n        if(np.argmax(sentiment) == 0):\n            predictions_dl.append(\"negative\")\n        elif (np.argmax(sentiment) == 1):\n            predictions_dl.append(\"positive\")\n    elif index > 332:\n        twt = comment\n        tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n        tokenizer.fit_on_texts(youtube_comments['correct_length'].values)\n        twt = tokenizer.texts_to_sequences(twt)\n        #padding the tweet to have exactly the same shape as `embedding_2` input\n        twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n        #print(twt)\n        sentiment = modely.predict(twt,batch_size=1,verbose = 2)[0]\n        if(np.argmax(sentiment) == 0):\n            predictions_dl.append(\"negative\")\n        elif (np.argmax(sentiment) == 1):\n            predictions_dl.append(\"positive\")\n        \n        \nprint(len(predictions))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:35:04.411625Z","iopub.execute_input":"2021-08-09T04:35:04.412105Z","iopub.status.idle":"2021-08-09T04:43:31.718702Z","shell.execute_reply.started":"2021-08-09T04:35:04.412065Z","shell.execute_reply":"2021-08-09T04:43:31.717748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testingdf.info()\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:44:48.469186Z","iopub.execute_input":"2021-08-09T04:44:48.469585Z","iopub.status.idle":"2021-08-09T04:44:48.486766Z","shell.execute_reply.started":"2021-08-09T04:44:48.469549Z","shell.execute_reply":"2021-08-09T04:44:48.48523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_of_g_c = testingdf['comments'].values\nlist_of_g_c = list_of_g_c.tolist()\n#list_of_g_c = list_of_g_c.asarray()\npredictions = []\nfor comment in list_of_g_c:\n    index = list_of_g_c.index(comment)\n    if index >= 0 & index < 166:\n        prediction = comment\n        prediction = [prediction]\n        prediction=tvt.transform(prediction)\n       \n        svc_tfidf_predict = svct.predict(prediction)\n        print(svc_tfidf_predict)\n        predictions.append(svc_tfidf_predict)\n    elif index >= 166 & index < 333:\n        prediction = comment\n        prediction = [prediction]\n        prediction=tvr.transform(prediction)\n       \n        svc_tfidf_predict = svcr.predict(prediction)\n        predictions.append(svc_tfidf_predict)\n    elif index > 332:\n        prediction = comment\n        prediction = [prediction]\n        prediction=tvy.transform(prediction)\n       \n        svc_tfidf_predict = svcy.predict(prediction)\n        predictions.append(svc_tfidf_predict)\n        \n        \nprint(len(predictions))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:29:11.668863Z","iopub.execute_input":"2021-08-09T05:29:11.669311Z","iopub.status.idle":"2021-08-09T05:29:13.942563Z","shell.execute_reply.started":"2021-08-09T05:29:11.669269Z","shell.execute_reply":"2021-08-09T05:29:13.941431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testingdf['sm_predictions_traditional'] = predictions","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:20:14.449118Z","iopub.execute_input":"2021-08-09T04:20:14.449502Z","iopub.status.idle":"2021-08-09T04:20:14.455063Z","shell.execute_reply.started":"2021-08-09T04:20:14.449469Z","shell.execute_reply":"2021-08-09T04:20:14.453879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testingdf['generic_predictions_traditional'] = predictions_g","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:22:01.175311Z","iopub.execute_input":"2021-08-09T04:22:01.17587Z","iopub.status.idle":"2021-08-09T04:22:01.180468Z","shell.execute_reply.started":"2021-08-09T04:22:01.175821Z","shell.execute_reply":"2021-08-09T04:22:01.179446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_g = []\nfor comment in list_of_g_c:\n    prediction = comment\n    prediction = [prediction]\n    prediction=tvg.transform(prediction)\n       \n    mnb_tfidf_predict = svc.predict(prediction)\n    predictions_g.append(mnb_tfidf_predict)\n    \n    twt = comment\n    tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n    tokenizer.fit_on_texts(generic_sentimentredo['comments'].values)\n    twt = tokenizer.texts_to_sequences(twt)\n    #padding the tweet to have exactly the same shape as `embedding_2` input\n    twt = pad_sequences(twt, maxlen=28, dtype='int32', value=0)\n    #print(twt)\n    sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n    if(np.argmax(sentiment) == 0):\n            predictions_dl.append(\"negative\")\n    elif (np.argmax(sentiment) == 1):\n            predictions_dl.append(\"positive\")\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:20.671552Z","iopub.execute_input":"2021-08-09T04:21:20.672099Z","iopub.status.idle":"2021-08-09T04:21:22.345902Z","shell.execute_reply.started":"2021-08-09T04:21:20.67206Z","shell.execute_reply":"2021-08-09T04:21:22.345071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_g = []\nfor comment in list_of_g_c:\n    prediction = comment\n    prediction = [prediction]\n    prediction=tvg.transform(prediction)\n       \n    mnb_tfidf_predict = mnbg.predict(prediction)\n    predictions_g.append(mnb_tfidf_predict)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:10.213208Z","iopub.execute_input":"2021-08-09T04:21:10.213598Z","iopub.status.idle":"2021-08-09T04:21:10.250154Z","shell.execute_reply.started":"2021-08-09T04:21:10.213567Z","shell.execute_reply":"2021-08-09T04:21:10.24807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(predictions_g))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T04:21:23.871016Z","iopub.execute_input":"2021-08-09T04:21:23.871832Z","iopub.status.idle":"2021-08-09T04:21:23.879208Z","shell.execute_reply.started":"2021-08-09T04:21:23.871774Z","shell.execute_reply":"2021-08-09T04:21:23.87777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"some other models idk","metadata":{}},{"cell_type":"code","source":"twitter_sample.info()\n","metadata":{"execution":{"iopub.status.busy":"2021-07-14T16:19:57.590543Z","iopub.execute_input":"2021-07-14T16:19:57.590981Z","iopub.status.idle":"2021-07-14T16:19:57.606952Z","shell.execute_reply.started":"2021-07-14T16:19:57.590943Z","shell.execute_reply":"2021-07-14T16:19:57.605864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reddit_sample['sentiment'] = reddit_sample['sentiment'].replace('positive', 1)\nreddit_sample['sentiment'] = reddit_sample['sentiment'].replace('negative', 0)\n\ntwitter_sample['sentiment'] = twitter_sample['sentiment'].replace('positive', 1)\ntwitter_sample['sentiment'] = twitter_sample['sentiment'].replace('negative', 0)\ntwitter_sample['sentiment'] = twitter_sample['sentiment'].replace('Positive', 1)\ntwitter_sample['sentiment'] = twitter_sample['sentiment'].replace('Negative', 0)\n\ngeneric_sample['sentiment'] = generic_sample['sentiment'].replace('positive', 1)\ngeneric_sample['sentiment'] = generic_sample['sentiment'].replace('negative', 0)\ngeneric_sample['sentiment'] = generic_sample['sentiment'].replace('Positive', 1)\ngeneric_sample['sentiment'] = generic_sample['sentiment'].replace('Negative', 0)\n#generic_sample.head(5)\n#twitter_sample.head(5)\n#reddit_sample.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T14:53:57.595274Z","iopub.execute_input":"2021-07-14T14:53:57.59562Z","iopub.status.idle":"2021-07-14T14:53:57.625073Z","shell.execute_reply.started":"2021-07-14T14:53:57.595592Z","shell.execute_reply":"2021-07-14T14:53:57.624277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nimport re,string,unicodedata\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem import LancasterStemmer,WordNetLemmatizer\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import svm\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport keras\nfrom keras.layers import Dense,LSTM\nfrom keras.models import Sequential","metadata":{"execution":{"iopub.status.busy":"2021-08-09T01:47:05.24272Z","iopub.execute_input":"2021-08-09T01:47:05.24332Z","iopub.status.idle":"2021-08-09T01:47:05.368184Z","shell.execute_reply.started":"2021-08-09T01:47:05.243267Z","shell.execute_reply":"2021-08-09T01:47:05.367377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generic_sentimentredo\ntwitter_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T01:52:56.17181Z","iopub.execute_input":"2021-08-09T01:52:56.17226Z","iopub.status.idle":"2021-08-09T01:52:56.189834Z","shell.execute_reply.started":"2021-08-09T01:52:56.172227Z","shell.execute_reply":"2021-08-09T01:52:56.188706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coms = generic_sentimentredo['comments'].values\nprint(type(coms[5000]))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:04:09.709238Z","iopub.execute_input":"2021-08-09T02:04:09.709849Z","iopub.status.idle":"2021-08-09T02:04:09.716787Z","shell.execute_reply.started":"2021-08-09T02:04:09.709797Z","shell.execute_reply":"2021-08-09T02:04:09.715516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coms = twitter_sample['correct_size'].values\nprint(type(coms[5000]))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T01:58:19.294407Z","iopub.execute_input":"2021-08-09T01:58:19.294771Z","iopub.status.idle":"2021-08-09T01:58:19.300157Z","shell.execute_reply.started":"2021-08-09T01:58:19.294738Z","shell.execute_reply":"2021-08-09T01:58:19.29909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sents = generic_sentimentredo['sentiment'].values\nprint(type(sents[5000]))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T01:59:03.329539Z","iopub.execute_input":"2021-08-09T01:59:03.330179Z","iopub.status.idle":"2021-08-09T01:59:03.337307Z","shell.execute_reply.started":"2021-08-09T01:59:03.330125Z","shell.execute_reply":"2021-08-09T01:59:03.336006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unused = []\nfor stra in coms:\n    \n    strastring = str(stra)\n    unused.append(strastring)\n    \n#print(len(unused))","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:04:36.504083Z","iopub.execute_input":"2021-08-09T02:04:36.504474Z","iopub.status.idle":"2021-08-09T02:04:36.513147Z","shell.execute_reply.started":"2021-08-09T02:04:36.504439Z","shell.execute_reply":"2021-08-09T02:04:36.512239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twitter_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T03:55:14.4402Z","iopub.execute_input":"2021-08-09T03:55:14.440589Z","iopub.status.idle":"2021-08-09T03:55:14.456649Z","shell.execute_reply.started":"2021-08-09T03:55:14.440557Z","shell.execute_reply":"2021-08-09T03:55:14.455251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"coms = twitter_sample['correct_size'].values\nsents = twitter_sample['sentiment'].values\n\n\nunused = []\nfor stra in coms:\n    \n    strastring = str(stra)\n    unused.append(strastring)\n\nx_train,x_test,y_train,y_test = train_test_split(unused, sents,test_size = 0.2 , random_state = 0)\n\ntvt=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n#transformed train reviews\ntv_train_reviews=tvt.fit_transform(x_train)\n#transformed test reviews\ntv_test_reviews=tvt.transform(x_test)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)\n\nsvct=SVC()\nsvct.fit(tv_train_reviews,y_train)\n\nsvc_tfidf_predict=svct.predict(tv_test_reviews)\n#Accuracy score for tfidf features\nsvc_tfidf_score=accuracy_score(y_test,svc_tfidf_predict)\nprint(\"twitter svc _tfidf_score :\",svc_tfidf_score)\n\ncoms = youtube_comments['correct_length'].values\nsents = youtube_comments['comp_score'].values\n\n\nunused = []\nfor stra in coms:\n    \n    strastring = str(stra)\n    unused.append(strastring)\n\nx_train,x_test,y_train,y_test = train_test_split(unused, sents,test_size = 0.2 , random_state = 0)\n\ntvy=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n#transformed train reviews\ntv_train_reviews=tvy.fit_transform(x_train)\n#transformed test reviews\ntv_test_reviews=tvy.transform(x_test)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)\n\nsvcy=SVC()\nsvcy.fit(tv_train_reviews,y_train)\n\nsvc_tfidf_predict=svcy.predict(tv_test_reviews)\n#Accuracy score for tfidf features\nsvc_tfidf_score=accuracy_score(y_test,svc_tfidf_predict)\nprint(\"youtube svc _tfidf_score :\",svc_tfidf_score)\n\n\n\ncoms = redditredo['comments'].values\nsents = redditredo['comp_score'].values\n\n\nunused = []\nfor stra in coms:\n    \n    strastring = str(stra)\n    unused.append(strastring)\n\nx_train,x_test,y_train,y_test = train_test_split(unused, sents,test_size = 0.2 , random_state = 0)\n\ntvr=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n#transformed train reviews\ntv_train_reviews=tvr.fit_transform(x_train)\n#transformed test reviews\ntv_test_reviews=tvr.transform(x_test)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)\n\nsvcr=SVC()\nsvcr.fit(tv_train_reviews,y_train)\n\nsvc_tfidf_predict=svcr.predict(tv_test_reviews)\n#Accuracy score for tfidf features\nsvc_tfidf_score=accuracy_score(y_test,svc_tfidf_predict)\nprint(\"reddit svc _tfidf_score :\",svc_tfidf_score)\n\ncoms = generic_sentimentredo['comments'].values\nsents = generic_sentimentredo['sentiment'].values\n\n#coms = youtube_comments['correct_length'].values\n#sents = youtube_comments['comp_score'].values\nunused = []\nfor stra in coms:\n    \n    strastring = str(stra)\n    unused.append(strastring)\n\nx_train,x_test,y_train,y_test = train_test_split(unused, sents,test_size = 0.2 , random_state = 0)\n\ntvg=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n#transformed train reviews\ntv_train_reviews=tvg.fit_transform(x_train)\n#transformed test reviews\ntv_test_reviews=tvg.transform(x_test)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)\n\nsvcg=SVC()\nsvcg.fit(tv_train_reviews,y_train)\n\nsvc_tfidf_predict=svcg.predict(tv_test_reviews)\n#Accuracy score for tfidf features\nsvc_tfidf_score=accuracy_score(y_test,svc_tfidf_predict)\nprint(\"generic svc _tfidf_score :\",svc_tfidf_score)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:24:42.850025Z","iopub.execute_input":"2021-08-09T05:24:42.850419Z","iopub.status.idle":"2021-08-09T05:25:09.522475Z","shell.execute_reply.started":"2021-08-09T05:24:42.850382Z","shell.execute_reply":"2021-08-09T05:25:09.521193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#coms = generic_sentimentredo['comments'].values\n#sents = generic_sentimentredo['sentiment'].values\n\ncoms = twitter_sample['correct_size'].values\nsents = twitter_sample['sentiment'].values\n\n#coms = df['correct_size'].values\n#sents = df['comp_score'].values\n\n#coms = youtube_comments['correct_length'].values\n#sents = youtube_comments['comp_score'].values\nunused = []\nfor stra in coms:\n    \n    strastring = str(stra)\n    unused.append(strastring)\n    \n\n#coms = coms.asarray()\n#sents = sents.asarray()\n\nx_train,x_test,y_train,y_test = train_test_split(unused, sents,test_size = 0.2 , random_state = 0)\n\ntv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n#transformed train reviews\ntv_train_reviews=tv.fit_transform(x_train)\n#transformed test reviews\ntv_test_reviews=tv.transform(x_test)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)\n\nlr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=0)\n\n#Fitting the model for tfidf features\nlr_tfidf=lr.fit(tv_train_reviews,y_train)\nprint(lr_tfidf)\n\nlr_tfidf_predict=lr.predict(tv_test_reviews)\nlr_tfidf_score=accuracy_score(y_test,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)\n\n\n#training the model\nmnb=MultinomialNB()\n#fitting the nb for bag of words\n\n#fitting the nb for tfidf features\nmnb_tfidf=mnb.fit(tv_train_reviews,y_train)\nprint(mnb_tfidf)\n\nmnb_tfidf_predict=mnb.predict(tv_test_reviews)\n#Accuracy score for tfidf features\nmnb_tfidf_score=accuracy_score(y_test,mnb_tfidf_predict)\nprint(\"mnb_tfidf_score :\",mnb_tfidf_score)\n\nsvc=SVC()\nsvc.fit(tv_train_reviews,y_train)\n\nsvc_tfidf_predict=svc.predict(tv_test_reviews)\n#Accuracy score for tfidf features\nsvc_tfidf_score=accuracy_score(y_test,svc_tfidf_predict)\nprint(\"svc _tfidf_score :\",svc_tfidf_score)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T05:24:25.783768Z","iopub.execute_input":"2021-08-09T05:24:25.784203Z","iopub.status.idle":"2021-08-09T05:24:30.627523Z","shell.execute_reply.started":"2021-08-09T05:24:25.784167Z","shell.execute_reply":"2021-08-09T05:24:30.626476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"twitter_sample.info()","metadata":{"execution":{"iopub.status.busy":"2021-08-09T01:48:38.26305Z","iopub.execute_input":"2021-08-09T01:48:38.263416Z","iopub.status.idle":"2021-08-09T01:48:38.280493Z","shell.execute_reply.started":"2021-08-09T01:48:38.263386Z","shell.execute_reply":"2021-08-09T01:48:38.279206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(generic_sentimentredo['comments'],generic_sentimentredo['sentiment'],test_size = 0.2 , random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T01:49:07.249382Z","iopub.execute_input":"2021-08-09T01:49:07.249803Z","iopub.status.idle":"2021-08-09T01:49:07.258377Z","shell.execute_reply.started":"2021-08-09T01:49:07.249766Z","shell.execute_reply":"2021-08-09T01:49:07.257455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n#transformed train reviews\ncv_train_reviews=cv.fit_transform(x_train)\n#transformed test reviews\ncv_test_reviews=cv.transform(x_test)\n\nprint('BOW_cv_train:',cv_train_reviews.shape)\nprint('BOW_cv_test:',cv_test_reviews.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:11:40.37674Z","iopub.execute_input":"2021-08-09T02:11:40.377152Z","iopub.status.idle":"2021-08-09T02:11:40.763002Z","shell.execute_reply.started":"2021-08-09T02:11:40.377115Z","shell.execute_reply":"2021-08-09T02:11:40.761931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n#transformed train reviews\ntv_train_reviews=tv.fit_transform(x_train)\n#transformed test reviews\ntv_test_reviews=tv.transform(x_test)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:11:42.42621Z","iopub.execute_input":"2021-08-09T02:11:42.426601Z","iopub.status.idle":"2021-08-09T02:11:42.814355Z","shell.execute_reply.started":"2021-08-09T02:11:42.426566Z","shell.execute_reply":"2021-08-09T02:11:42.813553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=0)\n#Fitting the model for Bag of words\nlr_bow=lr.fit(cv_train_reviews,y_train)\nprint(lr_bow)\n#Fitting the model for tfidf features\nlr_tfidf=lr.fit(tv_train_reviews,y_train)\nprint(lr_tfidf)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:11:44.195916Z","iopub.execute_input":"2021-08-09T02:11:44.196795Z","iopub.status.idle":"2021-08-09T02:11:46.034153Z","shell.execute_reply.started":"2021-08-09T02:11:44.196753Z","shell.execute_reply":"2021-08-09T02:11:46.031679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting the model for bag of words\nlr_bow_predict=lr.predict(cv_test_reviews)\n##Predicting the model for tfidf features\nlr_tfidf_predict=lr.predict(tv_test_reviews)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:11:47.498555Z","iopub.execute_input":"2021-08-09T02:11:47.498925Z","iopub.status.idle":"2021-08-09T02:11:47.504086Z","shell.execute_reply.started":"2021-08-09T02:11:47.498879Z","shell.execute_reply":"2021-08-09T02:11:47.503126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Accuracy score for bag of words\nlr_bow_score=accuracy_score(y_test,lr_bow_predict)\nprint(\"lr_bow_score :\",lr_bow_score)\n#Accuracy score for tfidf features\nlr_tfidf_score=accuracy_score(y_test,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:11:49.130432Z","iopub.execute_input":"2021-08-09T02:11:49.130964Z","iopub.status.idle":"2021-08-09T02:11:49.145789Z","shell.execute_reply.started":"2021-08-09T02:11:49.130929Z","shell.execute_reply":"2021-08-09T02:11:49.144382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Classification report for bag of words\nlr_bow_report=classification_report(y_test,lr_bow_predict,target_names=['0','1'])\nprint(lr_bow_report)\n\n#Classification report for tfidf features\nlr_tfidf_report=classification_report(y_test,lr_tfidf_predict,target_names=['0','1'])\nprint(lr_tfidf_report)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:11:50.942785Z","iopub.execute_input":"2021-08-09T02:11:50.943296Z","iopub.status.idle":"2021-08-09T02:11:51.084638Z","shell.execute_reply.started":"2021-08-09T02:11:50.943264Z","shell.execute_reply":"2021-08-09T02:11:51.083012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#training the model\nmnb=MultinomialNB()\n#fitting the nb for bag of words\nmnb_bow=mnb.fit(cv_train_reviews,y_train)\nprint(mnb_bow)\n#fitting the nb for tfidf features\nmnb_tfidf=mnb.fit(tv_train_reviews,y_train)\nprint(mnb_tfidf)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:11:54.627336Z","iopub.execute_input":"2021-08-09T02:11:54.627719Z","iopub.status.idle":"2021-08-09T02:11:54.686322Z","shell.execute_reply.started":"2021-08-09T02:11:54.627687Z","shell.execute_reply":"2021-08-09T02:11:54.685227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pred_list = []\nprediction = \"Trump sucks ass\"\nprediction = [prediction]\nprediction=tv.transform(prediction)\n#prediction = str(prediction)\n#pred_list.append(prediction)\n#prediction = [prediction]\nmnb_tfidf_predict = mnb.predict(prediction)\nprint(mnb_tfidf_predict)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:11:56.198101Z","iopub.execute_input":"2021-08-09T02:11:56.198615Z","iopub.status.idle":"2021-08-09T02:11:56.208401Z","shell.execute_reply.started":"2021-08-09T02:11:56.198563Z","shell.execute_reply":"2021-08-09T02:11:56.207485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting the model for bag of words\nmnb_bow_predict=mnb.predict(cv_test_reviews)\n#Predicting the model for tfidf features\nmnb_tfidf_predict=mnb.predict(tv_test_reviews)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:12:00.530012Z","iopub.execute_input":"2021-08-09T02:12:00.53046Z","iopub.status.idle":"2021-08-09T02:12:00.537045Z","shell.execute_reply.started":"2021-08-09T02:12:00.530421Z","shell.execute_reply":"2021-08-09T02:12:00.535821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Accuracy score for bag of words\nmnb_bow_score=accuracy_score(y_test,mnb_bow_predict)\nprint(\"mnb_bow_score :\",mnb_bow_score)\n#Accuracy score for tfidf features\nmnb_tfidf_score=accuracy_score(y_test,mnb_tfidf_predict)\nprint(\"mnb_tfidf_score :\",mnb_tfidf_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mnb_bow_report = classification_report(y_test,mnb_bow_predict,target_names = ['0','1'])\nprint(mnb_bow_report)\nmnb_tfidf_report = classification_report(y_test,mnb_tfidf_predict,target_names = ['0','1'])\nprint(mnb_tfidf_report)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:12:06.56254Z","iopub.execute_input":"2021-08-09T02:12:06.563032Z","iopub.status.idle":"2021-08-09T02:12:06.676596Z","shell.execute_reply.started":"2021-08-09T02:12:06.562909Z","shell.execute_reply":"2021-08-09T02:12:06.675528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tensorflow model #2","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(units = 75 , activation = 'relu' , input_dim = tv_train_reviews.shape[1]))\nmodel.add(Dense(units = 50 , activation = 'relu'))\nmodel.add(Dense(units = 25 , activation = 'relu'))\nmodel.add(Dense(units = 10 , activation = 'relu')) \nmodel.add(Dense(units = 1 , activation = 'sigmoid'))\nmodel.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n\nmodel.summary()\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:16:34.153592Z","iopub.execute_input":"2021-08-09T02:16:34.154017Z","iopub.status.idle":"2021-08-09T02:16:34.239516Z","shell.execute_reply.started":"2021-08-09T02:16:34.153981Z","shell.execute_reply":"2021-08-09T02:16:34.238696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#tv_train_reviews = tv_train_reviews\nmodel.fit(tv_train_reviews,y_train , epochs = 25)","metadata":{"execution":{"iopub.status.busy":"2021-08-09T02:16:36.054711Z","iopub.execute_input":"2021-08-09T02:16:36.055305Z","iopub.status.idle":"2021-08-09T02:16:36.674832Z","shell.execute_reply.started":"2021-08-09T02:16:36.055256Z","shell.execute_reply":"2021-08-09T02:16:36.672946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tv_test_reviews = tv_test_reviews.toarray()\nmodel.evaluate(tv_test_reviews,y_test)[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}