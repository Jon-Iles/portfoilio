{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport spacy\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-02T16:15:54.051216Z","iopub.execute_input":"2021-06-02T16:15:54.051626Z","iopub.status.idle":"2021-06-02T16:15:56.442066Z","shell.execute_reply.started":"2021-06-02T16:15:54.051543Z","shell.execute_reply":"2021-06-02T16:15:56.441092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reading the full sample into the DataFrame full_sample. I decided to cut 'created_at' as I did not see an immediate use for it.","metadata":{}},{"cell_type":"code","source":"full_sample = pd.read_csv(\"/kaggle/input/tweets-data/twcs.csv\")\nfull_sample.drop(columns=['created_at'], inplace=True)\nEMBEDDING_FILE = '/kaggle/input/glove840b300dtxt/glove.840B.300d.txt'","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:13:10.769304Z","iopub.execute_input":"2021-05-31T18:13:10.769733Z","iopub.status.idle":"2021-05-31T18:13:23.914511Z","shell.execute_reply.started":"2021-05-31T18:13:10.769703Z","shell.execute_reply":"2021-05-31T18:13:23.913315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At the beginning of the project for the sake of load times I used the sample csv to test my functions and explore the dataset. I begun with loading the data into the kaggle document and some inital EDA.","metadata":{}},{"cell_type":"code","source":"sample = pd.read_csv(\"/kaggle/input/sample/sample.csv\")\nsample.info()\nsample.describe()\nsample.head(10)\nsample['author_id'].unique()\nsample.drop(columns=['created_at'], inplace=True)\nsample.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.187767Z","iopub.status.idle":"2021-05-20T16:55:46.18817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is my full preprocessing condensed into one segment for easy of use. Full steps are below. This will take awhile, feel free to reduce the question_f.head(5000) to something smaller. It should all work fine. I hope. For reference my computer takes just over 3 minutes.","metadata":{}},{"cell_type":"code","source":"full_sample = pd.read_csv(\"/kaggle/input/tweets-data/twcs.csv\")\nfull_sample.drop(columns=['created_at'], inplace=True)\n\nquestions = full_sample[full_sample[\"inbound\"] == True]\n#questions.head()\nanswers = full_sample[full_sample[\"inbound\"] == False]\n#answers.head()\n\nquestion_f = pd.DataFrame(data=questions, columns = ['tweet_id', 'author_id', 'inbound', 'text', 'response_tweet_id', 'in_response_to_tweet_id'])\nanswer_f = pd.DataFrame(data=answers)\n#question_f\n\nquestion_f = question_f[question_f['response_tweet_id'].notna()]\n#sample['text']\nquestion_f = question_f.head(5000)\n#question_f.info()\n\nresponses = question_f['response_tweet_id']\n#print(responses)\nnew_responses = []\nchecker = \",\"\nfor response in responses: \n    if checker in response:\n        response_parts = response.split(\",\")\n        one_response = response_parts[0]\n        new_responses.append(one_response)\n    else:\n        new_responses.append(response)   \n    \n#print(len(new_responses))\n\n#print(new_responses)\nprint(\"1/5\")\ntweets = []\n#error = 'Series([], Name: text, dtype: object)'\nfor r in new_responses:\n    \n    #print(type(r))\n    number = int(r)\n    tweet = answer_f[answer_f['tweet_id'] == number]['text'].values\n    #print((tweet))\n    if len(tweet) == 0:\n        tweet = None\n    tweets.append(tweet)\n    \n    \nquestion_f['response_tweet'] = tweets\n#question_f = question_f.drop(question_f[question_f.response_tweet == None].index)\n#question_f.replace({'NaN': None})\nquestion_f = question_f[question_f['response_tweet'].notna()]\n#question_f\n\nprint(\"2/5\")\n!pip install spacy_cld\n\nimport spacy\nfrom spacy_cld import LanguageDetector\n\nnlp = spacy.load('en')\nlanguage_detector = LanguageDetector()\nnlp.add_pipe(language_detector)\n\ns = question_f['text'].values\nlang_list = []\n\nfor string in s:\n    doc = nlp(string)\n    \n    if(doc._.languages):\n        lang_list.append(doc._.languages[0])\n    # if it is empty, we append the list by unknown\n    else:\n        lang_list.append('unknown')\n\nquestion_f['languages_spacy'] = lang_list\n\nquestion_f = question_f.drop(question_f[question_f.languages_spacy != 'en'].index)\n\nprint(\"3/5\")\n\nimport emoji\nimport spacy\nimport stop_words\ns = question_f['text'].values\n#print(s)\ndef give_emoji_free_text(text):\n    return emoji.get_emoji_regexp().sub(r'', text)\n\nlist_of_list = []\n\nnlp = spacy.load(\"en_core_web_sm\")\nfor string in s:\n    string = string.lower()\n    string = give_emoji_free_text(string)\n    my_list = \"\"\n    doc = nlp(string)\n    #print(doc)\n   #print('1')\n    for token in doc:\n        if token.text[0:8] == \"https://\":\n            my_list += 'hyperlink' + ' '\n        elif token.text[0:1] == \"@\":\n            my_list += 'company' + ' '\n        else:\n            lemm = token.lemma_\n            poss = token.pos_\n            stopp = token.is_stop\n            thisdict = {\n                \"lem\": lemm,\n                \"pos\": poss,\n                \"stop\": stopp\n            }\n            if (thisdict[\"stop\"] == False) & ((thisdict[\"pos\"] == 'PROPN') or (thisdict[\"pos\"] == 'VERB') or (thisdict[\"pos\"] == 'NOUN') or (thisdict[\"pos\"] == 'NUM') or (thisdict[\"pos\"] == 'ADV') or (thisdict[\"pos\"] == 'ADJ')):\n                my_list += thisdict[\"lem\"] + ' '\n       \n    list_of_list.append(my_list)\n\nprint(\"4/5\")\n#print(list_of_list)\nquestion_f['processing_inti'] = list_of_list\n#question_f.head()\n\nquestion_f['response_tweet'] = question_f['response_tweet'].astype(str)\ns = question_f['response_tweet']\n#print(type(s))\n\ndef give_emoji_free_text(text):\n    return emoji.get_emoji_regexp().sub(r'', text)\n\nlist_of_lists = []\n\nnlp = spacy.load(\"en_core_web_sm\")\nfor string in s:\n    #print(string)\n    string = string.lower()\n    string = give_emoji_free_text(string)\n    my_list = \"\"\n    doc = nlp(string)\n    #print(doc)\n   #print('1')\n    for token in doc:\n        if token.text[0:8] == \"https://\":\n            my_list += 'hyperlink' + ' '\n        elif token.text[0:1] == \"@\":\n            my_list += 'user' + ' '\n        else:\n            lemm = token.lemma_\n            poss = token.pos_\n            stopp = token.is_stop\n            thisdict = {\n                \"lem\": lemm,\n                \"pos\": poss,\n                \"stop\": stopp\n            }\n            if (thisdict[\"stop\"] == False) & ((thisdict[\"pos\"] == 'PROPN') or (thisdict[\"pos\"] == 'VERB') or (thisdict[\"pos\"] == 'NOUN') or (thisdict[\"pos\"] == 'NUM') or (thisdict[\"pos\"] == 'ADV') or (thisdict[\"pos\"] == 'ADJ')):\n                my_list += thisdict[\"lem\"] + ' '\n       \n    list_of_lists.append(my_list)\n    \n#print(list_of_list)\nquestion_f['processing_resp'] = list_of_lists\nprint('5/5')\n\n#cleaning up the dataframe a bit\nquestion_f.drop(columns=['inbound'], inplace=True)\nquestion_f.drop(columns=['in_response_to_tweet_id'], inplace=True)\nquestion_f.drop(columns=['languages_spacy'], inplace=True)\nquestion_f.drop(columns=['response_tweet_id'], inplace=True)\nquestion_f.drop(columns=['author_id'], inplace=True)\nquestion_f.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T21:51:34.236301Z","iopub.execute_input":"2021-06-01T21:51:34.236712Z","iopub.status.idle":"2021-06-01T21:55:49.500966Z","shell.execute_reply.started":"2021-06-01T21:51:34.236677Z","shell.execute_reply":"2021-06-01T21:55:49.499603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Word cloud to explore common words in the processed question-answer pairs.","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\n#mpl.rcParams['figure.figsize']=(8.0,6.0)    #(6.0,4.0)\nmpl.rcParams['font.size']=12                #10 \nmpl.rcParams['savefig.dpi']=100             #72 \nmpl.rcParams['figure.subplot.bottom']=.1 \n\n\nstopwords = set(STOPWORDS)\ndata = question_f['text'].values\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=50,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(str(question_f['processing_resp'].values))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=900)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T20:59:21.010589Z","iopub.execute_input":"2021-05-31T20:59:21.010982Z","iopub.status.idle":"2021-05-31T20:59:22.365127Z","shell.execute_reply.started":"2021-05-31T20:59:21.010949Z","shell.execute_reply":"2021-05-31T20:59:22.364079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_f.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:29:25.995346Z","iopub.execute_input":"2021-06-01T19:29:25.995713Z","iopub.status.idle":"2021-06-01T19:29:26.012632Z","shell.execute_reply.started":"2021-06-01T19:29:25.995683Z","shell.execute_reply":"2021-06-01T19:29:26.01153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"At this point I was a bit confused about what form the data I needed was supposed to be in. I found an FAQ dataset for an E-learning system used for a japanese university. This chatbot took in three datasets: questions, answers and catagories. I'm unsure on how to I'll do the catagories but the first two should be easy enough. I noticed that \"inbound\" could be used to create question - answer pairs. There were some NaN values in 'response_tweet_id' so I just removed them.","metadata":{}},{"cell_type":"code","source":"questions = full_sample[full_sample[\"inbound\"] == True]\n#questions.head()\nanswers = full_sample[full_sample[\"inbound\"] == False]\nanswers.head()\n\nquestion_f = pd.DataFrame(data=questions, columns = ['tweet_id', 'author_id', 'inbound', 'text', 'response_tweet_id', 'in_response_to_tweet_id'])\nanswer_f = pd.DataFrame(data=answers)\nquestion_f\n\nquestion_f = question_f[question_f['response_tweet_id'].notna()]\n#sample['text']\nquestion_f = question_f.head(5000)\nquestion_f.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:39:19.594623Z","iopub.execute_input":"2021-05-31T18:39:19.59497Z","iopub.status.idle":"2021-05-31T18:39:22.527732Z","shell.execute_reply.started":"2021-05-31T18:39:19.594942Z","shell.execute_reply":"2021-05-31T18:39:22.527013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I'm now going to pair the tweet with the reponse_tweet_id to create a question - answer pair for my chatbot. Some of the tweets have multiple responses, annyoing, I can either create new rows to form new question-response pairs or remove one of the reponses. I know which one is easier but I'll try creating new pairs first. \nAfter more thought, I realised these duplicates would mess with the vocab building, so I'm opting for the dirty method and will remove the excess responses. Aka, create a new list of response_tweet_id's using the first id in each row.\nThe objects were strings so manipulation should be easy enough.","metadata":{}},{"cell_type":"code","source":"responses = question_f['response_tweet_id']\n#print(responses)\nnew_responses = []\nchecker = \",\"\nfor response in responses: \n    if checker in response:\n        response_parts = response.split(\",\")\n        one_response = response_parts[0]\n        new_responses.append(one_response)\n    else:\n        new_responses.append(response)   \n    \n#print(len(new_responses))\n\n#print(new_responses)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:45:05.624192Z","iopub.execute_input":"2021-05-31T18:45:05.625089Z","iopub.status.idle":"2021-05-31T18:45:05.635992Z","shell.execute_reply.started":"2021-05-31T18:45:05.625033Z","shell.execute_reply":"2021-05-31T18:45:05.635038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now I just have to find the corrosponding tweets in my answers dataframe. Some of the response tweets weren't in the CSV file so they have to be removed. ","metadata":{}},{"cell_type":"code","source":"answer_f\none = answer_f[answer_f['tweet_id'] == 119240]['text']\nprint(one)","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:45:09.041967Z","iopub.execute_input":"2021-05-31T18:45:09.043039Z","iopub.status.idle":"2021-05-31T18:45:09.058481Z","shell.execute_reply.started":"2021-05-31T18:45:09.042946Z","shell.execute_reply":"2021-05-31T18:45:09.056898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#answer_f.head()\ntweets = []\nfor r in new_responses:\n    \n    #print(type(r))\n    number = int(r)\n    tweet = answer_f[answer_f['tweet_id'] == number]['text'].values\n    #print((tweet))\n    if len(tweet) == 0:\n        tweet = None\n    tweets.append(tweet)\n    \n#print(len(tweets))\n#print(tweets[1])\n#s = tweets[1]\n#print(s)\n\nquestion_f['response_tweet'] = tweets\n#question_f = question_f.drop(question_f[question_f.response_tweet == None].index)\n#question_f.replace({'NaN': None})\nquestion_f = question_f[question_f['response_tweet'].notna()]\n#question_f","metadata":{"execution":{"iopub.status.busy":"2021-05-31T18:45:11.720262Z","iopub.execute_input":"2021-05-31T18:45:11.720621Z","iopub.status.idle":"2021-05-31T18:45:21.051678Z","shell.execute_reply.started":"2021-05-31T18:45:11.720586Z","shell.execute_reply":"2021-05-31T18:45:21.050957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_f.info()","metadata":{"execution":{"iopub.status.busy":"2021-05-31T19:08:41.326825Z","iopub.execute_input":"2021-05-31T19:08:41.327318Z","iopub.status.idle":"2021-05-31T19:08:41.347052Z","shell.execute_reply.started":"2021-05-31T19:08:41.327278Z","shell.execute_reply":"2021-05-31T19:08:41.345549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I noticed later on that there were some foreign languages in the twitter database. Of course this won't be much help to my chatbot so I can either translate them or delete them. ","metadata":{}},{"cell_type":"code","source":"!pip install spacy_cld","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.196676Z","iopub.status.idle":"2021-05-20T16:55:46.197058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a simple loop that creates a list with the primary language of each 'doc' aka tweet that I will use to remove all but English.","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport re\nimport spacy\nfrom spacy_cld import LanguageDetector\n\nnlp = spacy.load('en')\n#language_detector = LanguageDetector()\nnlp.add_pipe(language_detector)\n\ns = question_f['text'].values\nlang_list = []\n\nfor string in s:\n    doc = nlp(string)\n    \n    if(doc._.languages):\n        lang_list.append(doc._.languages[0])\n    # if it is empty, we append the list by unknown\n    else:\n        lang_list.append('unknown')\n\nquestion_f['languages_spacy'] = lang_list\n","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.19807Z","iopub.status.idle":"2021-05-20T16:55:46.198483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After finding a number of small occurances outside of the main few it seems easier to remove all but the 'en', English, tag. I used the built in drop function for this.","metadata":{}},{"cell_type":"code","source":"question_f['languages_spacy'].value_counts()\nquestion_f = question_f.drop(question_f[question_f.languages_spacy != 'en'].index)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.199403Z","iopub.status.idle":"2021-05-20T16:55:46.199822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now onto tagging and categorizing the words. I started by using the method you showed us using NLTK, confused myself trying to manipulate strings, and ended up building my own method using spacy. This method removes stopwords, punctuation and symbols and provides lemmatization. I came back to change this process from removing punctuation and symbols to keeping nouns, verbs, proper nouns and numbers. I'm unsure whether this makes the remove stopwords redundant but I can't harm to keep it in. (except in computing time; oops)","metadata":{}},{"cell_type":"code","source":"import emoji\nimport spacy\nimport stop_words\ns = question_f['text']\n#print(s)\ndef give_emoji_free_text(text):\n    return emoji.get_emoji_regexp().sub(r'', text)\n\nlist_of_list = []\n\nnlp = spacy.load(\"en_core_web_sm\")\nfor string in s:\n    string = string.lower()\n    string = give_emoji_free_text(string)\n    my_list = \"\"\n    doc = nlp(string)\n    #print(doc)\n   #print('1')\n    for token in doc:\n        if token.text[0:8] == \"https://\":\n            my_list += 'hyperlink' + ' '\n        #print(type(token))\n        elif token.text[0:1] == \"@\":\n            my_list += token.text[1:] + ' '\n        else:\n            lemm = token.lemma_\n            poss = token.pos_\n            stopp = token.is_stop\n            thisdict = {\n                \"lem\": lemm,\n                \"pos\": poss,\n                \"stop\": stopp\n            }\n            if (thisdict[\"stop\"] == False) & ((thisdict[\"pos\"] == 'PROPN') or (thisdict[\"pos\"] == 'VERB') or (thisdict[\"pos\"] == 'NOUN') or (thisdict[\"pos\"] == 'NUM')):\n                my_list += thisdict[\"lem\"] + ' '\n       \n    list_of_list.append(my_list)\n    \n#print(list_of_list)\nquestion_f['processing_inti'] = list_of_list\nquestion_f.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.200752Z","iopub.status.idle":"2021-05-20T16:55:46.201169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is the same method but for the responses. I could combine them to create a function but this is low on my TODO list.","metadata":{}},{"cell_type":"code","source":"import emoji\nimport spacy\nimport stop_words\nquestion_f['response_tweet'] = question_f['response_tweet'].astype(str)\ns = question_f['response_tweet']\n#print(type(s))\n\ndef give_emoji_free_text(text):\n    return emoji.get_emoji_regexp().sub(r'', text)\n\nlist_of_lists = []\n\nnlp = spacy.load(\"en_core_web_sm\")\nfor string in s:\n    #print(string)\n    string = string.lower()\n    string = give_emoji_free_text(string)\n    my_list = \"\"\n    doc = nlp(string)\n    #print(doc)\n   #print('1')\n    for token in doc:\n        if token.text[0:8] == \"https://\":\n            my_list += 'hyperlink' + ' '\n        elif token.text[0:1] == \"@\":\n            my_list += 'user' + ' '        \n        else:\n            lemm = token.lemma_\n            poss = token.pos_\n            stopp = token.is_stop\n            thisdict = {\n                \"lem\": lemm,\n                \"pos\": poss,\n                \"stop\": stopp\n            }\n            if (thisdict[\"stop\"] == False) & ((thisdict[\"pos\"] == 'PROPN') or (thisdict[\"pos\"] == 'VERB') or (thisdict[\"pos\"] == 'NOUN') or (thisdict[\"pos\"] == 'NUM')):\n                my_list += thisdict[\"lem\"] + ' '\n       \n    list_of_lists.append(my_list)\n    \n#print(list_of_list)\nquestion_f['processing_resp'] = list_of_lists\nquestion_f.head()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.202058Z","iopub.status.idle":"2021-05-20T16:55:46.202505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With my question - answer pairs sorted I decided to create a very simple chatbot that \n1. Processes input data\n2. Loops through the existing data \n3. Compares the similarity of the two vector averages\n4. Finds an existing question with a similarity > 0.8\n5. Finds the corrosponding answer and prints it","metadata":{}},{"cell_type":"code","source":"import spacy\n#from spacy import en_core_web_sm\nflag = True\nquestions_t = list(question_f['processing_inti'].values)\nanswers_t = list(question_f['processing_resp'].values)\n\ndef process_string(user_input):\n    nlp = spacy.load(\"en_core_web_sm\")\n    my_list = \"\"\n    doc = nlp(user_input)   \n    for token in doc:        \n        lemm = token.lemma_\n        poss = token.pos_\n        stopp = token.is_stop\n        thisdict = {\n            \"lem\": lemm,\n            \"pos\": poss,\n            \"stop\": stopp\n        }\n        if (thisdict[\"stop\"] == False) & (thisdict[\"pos\"] != 'PUNCT') & (thisdict[\"pos\"] != 'SYM'):\n            my_list += thisdict[\"lem\"] + ' '\n       \n    return my_list\n\nprint('BOT: My name is ChatChat.v1, I will answer your questions about as best as i can. If you want to exit, type Bye')\n\nwhile flag:\n    user_response = input( 'Enter question : ' )\n    user_response = user_response.lower()\n    print(\"User: \", user_response)\n    answered = 0\n    if user_response is not 'bye':\n        processed_input = process_string(user_response)\n        \n        doc1 = nlp(processed_input)\n        for quest in questions_t:\n            doc2 = nlp(quest)\n            simil = doc1.similarity(doc2)\n            #add randomiser to retrieve random responses instead of the first one\n            if simil > 0.80:\n                index = questions_t.index(quest)\n                print(\"ChatChat: \", answers_t[index])\n                answered = 1\n                break;\n        if answered == 0:\n            print(\"Sorry I can't understand you. Please try again.\")        \n    else:\n        print('ChatChat: bye!')\n        flag = False","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:52:05.790609Z","iopub.execute_input":"2021-06-01T19:52:05.791019Z","iopub.status.idle":"2021-06-01T19:52:19.114643Z","shell.execute_reply.started":"2021-06-01T19:52:05.790982Z","shell.execute_reply":"2021-06-01T19:52:19.112006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I will now create a more advanced chatbot, building from what we have.","metadata":{}},{"cell_type":"markdown","source":"First I'm going to quickly edit my list of inputs to a max length = 20 tokens, min length = 3 tokens.","metadata":{}},{"cell_type":"code","source":"sentences = list(question_f['processing_inti'].values)\nanswers_pre =  list(question_f['processing_resp'].values)\nmax_length = 21\nmin_length = 2\nquestions = list()\nanswers = list()\nfor sent in sentences:\n    length = len(sent.split())\n    #print(len(tokens))\n    if length > min_length & length < max_length:\n        index = sentences.index(sent)\n        questions.append(sentences[index])\n        answers.append(answers_pre[index])\n        \n#print(len(questions))\n#print(len(answers))\nquestions_tokened = list()\nfor q in questions:    \n    tokens = list(q.split())\n    questions_tokened.append(tokens)\n    \nprint(len(questions_tokened))\nprint(questions_tokened[1])","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:55:43.74819Z","iopub.execute_input":"2021-06-01T19:55:43.748569Z","iopub.status.idle":"2021-06-01T19:55:44.138267Z","shell.execute_reply.started":"2021-06-01T19:55:43.748534Z","shell.execute_reply":"2021-06-01T19:55:44.137511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are two methods for my next step, i'll either use doc2vec or tfidfvectorizer. To convert the text into numeric data.","metadata":{}},{"cell_type":"markdown","source":"Doc2Vec","metadata":{}},{"cell_type":"code","source":"import smart_open\nimport gensim\nfrom gensim import utils\nfrom gensim.models import Doc2Vec\n\nfrom sklearn.model_selection import train_test_split\nX = questions\ny = answers\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ntrain_list = list()\nfor i, s in enumerate(X_train):\n    tokens = gensim.utils.simple_preprocess(s)\n    p_tokens = gensim.models.doc2vec.TaggedDocument(tokens, [i])\n    train_list.append(p_tokens)\n\nprint(train_list)\nmodel = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\nmodel.build_vocab(train_list)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.206178Z","iopub.status.idle":"2021-05-20T16:55:46.206647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tfidfvectorizer:","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n# list of text documents\ntext = questions\n# create the transform\nvectorizer = TfidfVectorizer()\n# tokenize and build vocab\nX = vectorizer.fit(text)\n# summarize\n#print(vectorizer.vocabulary_)\nprint(X.idf_)\n# encode document\nvector = X.transform([text[0]])\n# summarize encoded vector\nprint(vector.shape)\nprint(vector.toarray())\n\nbigv = X.transform(questions)\n\nlist_of_vectors = list()\n\nfor t in text:\n    vector = X.transform([t])\n    array = vector.toarray()\n    list_of_vectors.append(array)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:55:48.169703Z","iopub.execute_input":"2021-06-01T19:55:48.170326Z","iopub.status.idle":"2021-06-01T19:55:50.872126Z","shell.execute_reply.started":"2021-06-01T19:55:48.17027Z","shell.execute_reply":"2021-06-01T19:55:50.871169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I have a better grasp of tfidvectorizer, and the frequency weighting is supposed to be better so I will use this method going forward. Next I will test and implement a k-means algorithm. Early on in this assingment I tested an alternative method using gensim and a LDA model, which you can see at the bottom of the document with the rest of my failed experiments, unfortunatly I couldn't get everything working satifactory.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\ntrue_k = 21\nmodel = KMeans(n_clusters=true_k)\nmodel.fit(bigv)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.209072Z","iopub.status.idle":"2021-05-20T16:55:46.20954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For the clutering algorithm my researched suggested k-means was a solid method, I then experimented with kmeans and minibatchkmeans and their inputs to find the best result.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\ndef find_optimal_clusters(data, max_k):\n    iters = range(40, max_k+1, 2)\n    \n    sse = []\n    for k in iters:\n        sse.append(KMeans(n_clusters=k, random_state=40).fit(data).inertia_)\n        print('Fit {} clusters'.format(k))\n        #sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=1024, random_state=40).fit(data).inertia_)\n    f, ax = plt.subplots(1, 1)\n    ax.plot(iters, sse, marker='o')\n    ax.set_xlabel('Cluster Centers')\n    ax.set_xticks(iters)\n    ax.set_xticklabels(iters)\n    ax.set_ylabel('SSE')\n    ax.set_title('SSE by Cluster Center Plot K-means')\n    \nfind_optimal_clusters(bigv, 60)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.210516Z","iopub.status.idle":"2021-05-20T16:55:46.210932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With our ideal k-means cluster value found I can now classify these tweets into 60 categories!!! Hopefully this large size with enable somewhat accurate responses! It turns out k-means was slightly superior over minibatchkmeans. ","metadata":{}},{"cell_type":"markdown","source":"Converting each question to a tfdi vector and using my k-means model to assign them a category (cluster).","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nmodel = KMeans(n_clusters=60, random_state=42)\nmodel.fit(bigv)\n\nlist_of_clusters = list()\n\nfor q in questions:\n    vector = X.transform([q])\n    cluster = model.predict(vector)\n    list_of_clusters.append(cluster)\n    \n#print(list_of_clusters)\nprint(len(list_of_clusters))","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:55:59.426045Z","iopub.execute_input":"2021-06-01T19:55:59.426419Z","iopub.status.idle":"2021-06-01T19:56:22.268729Z","shell.execute_reply.started":"2021-06-01T19:55:59.426387Z","shell.execute_reply":"2021-06-01T19:56:22.267926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:57:44.633688Z","iopub.execute_input":"2021-06-01T19:57:44.634192Z","iopub.status.idle":"2021-06-01T19:57:50.423953Z","shell.execute_reply.started":"2021-06-01T19:57:44.634148Z","shell.execute_reply":"2021-06-01T19:57:50.422925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With my clusters assigned there was the matter of sorting these question-answer-cluster trios into groups for my chatbot to use. I used lists because they seemed like the easiest implementation. Basically I created lists of answers for each cluster id and then combined these into a list of lists. ","metadata":{}},{"cell_type":"code","source":"#more sophisticated retireval system\nlist_of_lists = list()\nlist_of_ans = list()\n\ni = 0\ni_list = list()\nwhile i < 60:\n    i_list.append(i)\n    i = i + 1\n\nprint(i_list)\n    \n\nfor i in i_list:\n    #print(i)    \n    \n    for q in questions:        \n        index = questions.index(q)\n        #print(index)\n        clusternumber = int(list_of_clusters[index])\n        on_cluser = int(i)\n        #print(clusternumber)\n        if clusternumber == on_cluser:                      \n            list_of_ans.append(answers[index])  \n    \n    #print(list_of_ans)\n    #print(i)\n    list_of_lists.append(list(list_of_ans))\n    #print(list_of_lists)\n    list_of_ans.clear()\n       \n#print(len(list_of_lists))\n#print(list_of_lists)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:57:58.914592Z","iopub.execute_input":"2021-06-01T19:57:58.91505Z","iopub.status.idle":"2021-06-01T19:58:04.518828Z","shell.execute_reply.started":"2021-06-01T19:57:58.915012Z","shell.execute_reply":"2021-06-01T19:58:04.517897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Splitting my data for training/testing and fitting the multinominal naive bayes model - which from my research seemed most appropriate. I also tested guassian (which is why the model name is GNB).","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(questions, list_of_clusters, test_size=0.2,random_state=42)\n\nX_traintfv = X.transform(X_train)\nX_testtfv = X.transform(X_test)\nX_traintfv_array = X_traintfv.toarray()\nX_testtfv_array = X_testtfv.toarray()\n\n\nmodelGNB = MultinomialNB()\nmodelGNB.fit(X_traintfv_array,y_train)\n#predictions = modelGNB.predict_proba(X_testtfv_array)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:58:04.70469Z","iopub.execute_input":"2021-06-01T19:58:04.705145Z","iopub.status.idle":"2021-06-01T19:58:05.026638Z","shell.execute_reply.started":"2021-06-01T19:58:04.705108Z","shell.execute_reply":"2021-06-01T19:58:05.025573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing the process my bot with use to understand user input. More details about this below.","metadata":{}},{"cell_type":"code","source":"from random import seed\nfrom random import randint\ndef process_string(user_input):\n    nlp = spacy.load(\"en_core_web_sm\")\n    my_list = \"\"\n    doc = nlp(user_input)   \n    for token in doc:        \n        lemm = token.lemma_\n        poss = token.pos_\n        stopp = token.is_stop\n        thisdict = {\n            \"lem\": lemm,\n            \"pos\": poss,\n            \"stop\": stopp\n        }\n        if (thisdict[\"stop\"] == False) & ((thisdict[\"pos\"] == 'PROPN') or (thisdict[\"pos\"] == 'VERB') or (thisdict[\"pos\"] == 'NOUN') or (thisdict[\"pos\"] == 'NUM') or (thisdict[\"pos\"] == 'ADV') or (thisdict[\"pos\"] == 'ADJ')):\n            my_list += thisdict[\"lem\"] + ' '\n       \n    return my_list\n\n\nuser_response = 'company you are so bad at customer response'\nprocessed_input = process_string(user_response)\ntfdi_input = X.transform([processed_input])\ntfdi_input_array = tfdi_input.toarray()\n\nprediction = modelGNB.predict(tfdi_input_array)\nprint(prediction)\n\nprediction = int(prediction)\nprint(prediction)\n        \npossible_answers = list_of_lists[prediction]\nprint(len(possible_answers))\nvalue = randint(0, len(possible_answers))\n        \nresponse = possible_answers[value]\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:58:08.260066Z","iopub.execute_input":"2021-06-01T19:58:08.260444Z","iopub.status.idle":"2021-06-01T19:58:09.429961Z","shell.execute_reply.started":"2021-06-01T19:58:08.260393Z","shell.execute_reply":"2021-06-01T19:58:09.428763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is my second chatbot iteration: this time featuring k-means clustering and multinomial naive bayes for classification.\n\n1) take input\n2) preprocess \n3) turn into tfid vector\n4) to array\n5) predict with multiNB","metadata":{}},{"cell_type":"code","source":"import spacy\nfrom random import seed\nfrom random import randint\n#from spacy import en_core_web_sm\nflag = True\n\n\ndef process_string(user_input):\n    nlp = spacy.load(\"en_core_web_sm\")\n    my_list = \"\"\n    doc = nlp(user_input)   \n    for token in doc:        \n        lemm = token.lemma_\n        poss = token.pos_\n        stopp = token.is_stop\n        thisdict = {\n            \"lem\": lemm,\n            \"pos\": poss,\n            \"stop\": stopp\n        }\n        if (thisdict[\"stop\"] == False) & ((thisdict[\"pos\"] == 'PROPN') or (thisdict[\"pos\"] == 'VERB') or (thisdict[\"pos\"] == 'NOUN') or (thisdict[\"pos\"] == 'NUM') or (thisdict[\"pos\"] == 'ADV') or (thisdict[\"pos\"] == 'ADJ')):\n            my_list += thisdict[\"lem\"] + ' '\n       \n    return my_list\n\nprint('BOT: My name is ChatChat.v2, I will answer your questions about as best as i can. If you want to exit, type Bye')\n\nwhile flag:\n    user_response = input( 'Enter question : ' )\n    user_response = user_response.lower()\n    print(\"User: \", user_response)\n    answered = 0\n    if user_response is not 'bye':\n        processed_input = process_string(user_response)\n        tfdi_input = X.transform([processed_input])\n        tfdi_input_array = tfdi_input.toarray()\n        prediction = modelGNB.predict(tfdi_input_array)\n        prediction = int(prediction)\n        \n        possible_answers = list_of_lists[prediction]\n        value = randint(0, len(possible_answers))\n        \n        response = possible_answers[value]\n        print(response)\n              \n    else:\n        print('ChatChat: bye!')\n        flag = False","metadata":{"execution":{"iopub.status.busy":"2021-06-01T19:58:16.633971Z","iopub.execute_input":"2021-06-01T19:58:16.634355Z","iopub.status.idle":"2021-06-01T20:06:26.767261Z","shell.execute_reply.started":"2021-06-01T19:58:16.634309Z","shell.execute_reply":"2021-06-01T20:06:26.765471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now for deep learning round 20. This is the first seq2seq model I got to work but about my fourth attempt. To start I will edit down the input strings to a certain token length. ","metadata":{}},{"cell_type":"code","source":"sentences = list(question_f['processing_inti'].values)\nanswers_pre =  list(question_f['processing_resp'].values)\nmax_length = 21\nmin_length = 2\nquestions = list()\nanswers = list()\nfor sent in sentences:\n    length = len(sent.split())\n    #print(len(tokens))\n    if length > min_length & length < max_length:\n        index = sentences.index(sent)\n        questions.append(sentences[index])\n        answers.append(answers_pre[index])\n\npre_answer = answers","metadata":{"execution":{"iopub.status.busy":"2021-06-01T21:57:03.809145Z","iopub.execute_input":"2021-06-01T21:57:03.809572Z","iopub.status.idle":"2021-06-01T21:57:04.253419Z","shell.execute_reply.started":"2021-06-01T21:57:03.80954Z","shell.execute_reply":"2021-06-01T21:57:04.252181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I first built a joint question-answer vocabulary, then using the built in tokenizer and one hot encoding to prepare the training data for the seq2seq model. (includes pading etc)","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import preprocessing , utils\n\nanswers = list()\nfor i in range( len( pre_answer ) ) :\n    answers.append( '<START> ' + pre_answer[i] + ' <END>' )\n\ntokenizer = preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts( questions + answers )\nVOCAB_SIZE = len( tokenizer.word_index )+1\nprint( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n\n# encoder_input_data\ntokenized_questions = tokenizer.texts_to_sequences( questions )\nmaxlen_questions = max( [ len(x) for x in tokenized_questions ] )\npadded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\nencoder_input_data = np.array( padded_questions )\nprint( encoder_input_data.shape , maxlen_questions )\n\n# decoder_input_data\ntokenized_answers = tokenizer.texts_to_sequences( answers )\nmaxlen_answers = max( [ len(x) for x in tokenized_answers ] )\npadded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\ndecoder_input_data = np.array( padded_answers )\nprint( decoder_input_data.shape , maxlen_answers )\n\n# decoder_output_data\ntokenized_answers = tokenizer.texts_to_sequences( answers )\nfor i in range(len(tokenized_answers)) :\n    tokenized_answers[i] = tokenized_answers[i][1:]\npadded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\nonehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\ndecoder_output_data = np.array( onehot_answers )\nprint( decoder_output_data.shape )","metadata":{"execution":{"iopub.status.busy":"2021-06-01T21:57:06.9669Z","iopub.execute_input":"2021-06-01T21:57:06.967307Z","iopub.status.idle":"2021-06-01T21:57:20.401556Z","shell.execute_reply.started":"2021-06-01T21:57:06.967277Z","shell.execute_reply":"2021-06-01T21:57:20.400286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a functional seq2seq model.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nimport pickle\nfrom tensorflow.keras import layers , activations , models , preprocessing\n\nencoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\nencoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\nencoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\nencoder_states = [ state_h , state_c ]\n\ndecoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\ndecoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\ndecoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\ndecoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\ndecoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \noutput = decoder_dense ( decoder_outputs )\n\nmodel = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-06-01T21:57:26.234237Z","iopub.execute_input":"2021-06-01T21:57:26.234626Z","iopub.status.idle":"2021-06-01T21:57:28.126742Z","shell.execute_reply.started":"2021-06-01T21:57:26.234596Z","shell.execute_reply":"2021-06-01T21:57:28.125636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del answer_f\ndel question_f\ndel full_sample","metadata":{"execution":{"iopub.status.busy":"2021-06-01T21:57:30.294758Z","iopub.execute_input":"2021-06-01T21:57:30.295263Z","iopub.status.idle":"2021-06-01T21:57:30.468801Z","shell.execute_reply.started":"2021-06-01T21:57:30.295218Z","shell.execute_reply":"2021-06-01T21:57:30.467921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=70 )","metadata":{"execution":{"iopub.status.busy":"2021-06-01T21:57:33.691626Z","iopub.execute_input":"2021-06-01T21:57:33.69227Z","iopub.status.idle":"2021-06-01T23:01:30.301593Z","shell.execute_reply.started":"2021-06-01T21:57:33.692216Z","shell.execute_reply":"2021-06-01T23:01:30.300448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The inference model to retrieve output:","metadata":{}},{"cell_type":"code","source":"def make_inference_models():\n    \n    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n    \n    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n    \n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    \n    decoder_outputs, state_h, state_c = decoder_lstm(\n        decoder_embedding , initial_state=decoder_states_inputs)\n    decoder_states = [state_h, state_c]\n    decoder_outputs = decoder_dense(decoder_outputs)\n    decoder_model = tf.keras.models.Model(\n        [decoder_inputs] + decoder_states_inputs,\n        [decoder_outputs] + decoder_states)\n    \n    return encoder_model , decoder_model\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T23:01:52.693222Z","iopub.execute_input":"2021-06-01T23:01:52.693602Z","iopub.status.idle":"2021-06-01T23:01:52.703573Z","shell.execute_reply.started":"2021-06-01T23:01:52.69357Z","shell.execute_reply":"2021-06-01T23:01:52.702239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a function to prepare user input for the model.","metadata":{}},{"cell_type":"code","source":"def str_to_tokens( sentence : str ):\n    \n    nlp = spacy.load(\"en_core_web_sm\")\n    my_list = \"\"\n    doc = nlp(sentence)   \n    for token in doc:        \n        lemm = token.lemma_\n        poss = token.pos_\n        stopp = token.is_stop\n        thisdict = {\n            \"lem\": lemm,\n            \"pos\": poss,\n            \"stop\": stopp\n        }\n        if (thisdict[\"stop\"] == False) & ((thisdict[\"pos\"] == 'PROPN') or (thisdict[\"pos\"] == 'VERB') or (thisdict[\"pos\"] == 'NOUN') or (thisdict[\"pos\"] == 'NUM') or (thisdict[\"pos\"] == 'ADV') or (thisdict[\"pos\"] == 'ADJ')):\n            my_list += thisdict[\"lem\"] + ' '\n    words = my_list.lower().split()\n    tokens_list = list()\n    for word in words:\n        if tokenizer.word_index[ word ] > 0 & tokenizer.word_index[ word ] < VOCAB_SIZE:\n            tokens_list.append( tokenizer.word_index[ word ] ) \n    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')","metadata":{"execution":{"iopub.status.busy":"2021-06-01T23:01:56.977325Z","iopub.execute_input":"2021-06-01T23:01:56.977746Z","iopub.status.idle":"2021-06-01T23:01:56.987616Z","shell.execute_reply.started":"2021-06-01T23:01:56.977703Z","shell.execute_reply":"2021-06-01T23:01:56.986297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"My 3rd chatbot: featuring a seq2seq model. ","metadata":{}},{"cell_type":"code","source":"enc_model , dec_model = make_inference_models()\n\nfor _ in range(10):\n    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n    empty_target_seq = np.zeros( ( 1 , 1 ) )\n    empty_target_seq[0, 0] = tokenizer.word_index['start']\n    stop_condition = False\n    decoded_translation = ''\n    while not stop_condition :\n        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n        sampled_word = None\n        for word , index in tokenizer.word_index.items() :\n            if sampled_word_index == index :\n                decoded_translation += ' {}'.format( word )\n                sampled_word = word\n        \n        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n            stop_condition = True\n            \n        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n        empty_target_seq[ 0 , 0 ] = sampled_word_index\n        states_values = [ h , c ] \n\n    print( decoded_translation )\n","metadata":{"execution":{"iopub.status.busy":"2021-06-01T23:01:59.743203Z","iopub.execute_input":"2021-06-01T23:01:59.743897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this functional model complete I will try a sequential model with a glove vector embedding. First I will edit down my training data. I didn't end up using this model because I couldn't load as many rows of data to train the model, thus it performed worse. ","metadata":{}},{"cell_type":"code","source":"sentences = list(question_f['processing_inti'].values)\nanswers_pre =  list(question_f['processing_resp'].values)\nmax_length = 21\nmin_length = 2\nquestions = list()\nanswers = list()\nfor sent in sentences:\n    length = len(sent.split())\n    #print(len(tokens))\n    if length > min_length & length < max_length:\n        index = sentences.index(sent)\n        questions.append(sentences[index])\n        answers.append(answers_pre[index])\n\npre_answer = answers","metadata":{"execution":{"iopub.status.busy":"2021-05-27T14:42:32.25833Z","iopub.execute_input":"2021-05-27T14:42:32.258742Z","iopub.status.idle":"2021-05-27T14:42:33.583249Z","shell.execute_reply.started":"2021-05-27T14:42:32.258707Z","shell.execute_reply":"2021-05-27T14:42:33.582146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The same preprocessing as before:","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import preprocessing , utils\n\nanswers = list()\nfor i in range( len( pre_answer ) ) :\n    answers.append( '<START> ' + pre_answer[i] + ' <END>' )\n\ntokenizer = preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts( questions + answers )\nVOCAB_SIZE = len( tokenizer.word_index )+1\nprint( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n\n# encoder_input_data\ntokenized_questions = tokenizer.texts_to_sequences( questions )\nmaxlen_questions = max( [ len(x) for x in tokenized_questions ] )\npadded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\nencoder_input_data = np.array( padded_questions )\nprint( encoder_input_data.shape , maxlen_questions )\n\n# decoder_input_data\ntokenized_answers = tokenizer.texts_to_sequences( answers )\nmaxlen_answers = max( [ len(x) for x in tokenized_answers ] )\npadded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\ndecoder_input_data = np.array( padded_answers )\nprint( decoder_input_data.shape , maxlen_answers )\n\n# decoder_output_data\ntokenized_answers = tokenizer.texts_to_sequences( answers )\nfor i in range(len(tokenized_answers)) :\n    tokenized_answers[i] = tokenized_answers[i][1:]\npadded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\nonehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\ndecoder_output_data = np.array( onehot_answers )\nprint( decoder_output_data.shape )","metadata":{"execution":{"iopub.status.busy":"2021-05-27T14:42:34.98343Z","iopub.execute_input":"2021-05-27T14:42:34.98384Z","iopub.status.idle":"2021-05-27T14:42:40.403048Z","shell.execute_reply.started":"2021-05-27T14:42:34.983807Z","shell.execute_reply":"2021-05-27T14:42:40.40221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the embedding ready, I decided to go with the 840/300 as a suitible middle ground. ","metadata":{}},{"cell_type":"code","source":"EMBEDDING_FILE = '/kaggle/input/glove840b300dtxt/glove.840B.300d.txt'\n\ndef get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-05-27T14:51:23.800764Z","iopub.execute_input":"2021-05-27T14:51:23.801177Z","iopub.status.idle":"2021-05-27T14:51:23.842987Z","shell.execute_reply.started":"2021-05-27T14:51:23.801138Z","shell.execute_reply":"2021-05-27T14:51:23.841887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_features = len(word_index) - 1\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-05-27T14:48:12.071176Z","iopub.execute_input":"2021-05-27T14:48:12.071588Z","iopub.status.idle":"2021-05-27T14:48:19.09888Z","shell.execute_reply.started":"2021-05-27T14:48:12.07155Z","shell.execute_reply":"2021-05-27T14:48:19.097145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:06:05.880845Z","iopub.execute_input":"2021-05-27T15:06:05.881368Z","iopub.status.idle":"2021-05-27T15:06:05.889255Z","shell.execute_reply.started":"2021-05-27T15:06:05.881328Z","shell.execute_reply":"2021-05-27T15:06:05.888123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom tqdm import tqdm\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text","metadata":{"execution":{"iopub.status.busy":"2021-05-27T14:47:39.553966Z","iopub.execute_input":"2021-05-27T14:47:39.55442Z","iopub.status.idle":"2021-05-27T14:47:39.75506Z","shell.execute_reply.started":"2021-05-27T14:47:39.554374Z","shell.execute_reply":"2021-05-27T14:47:39.75385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(word_index) + 1,\n                     300,\n                     weights=[embedding_matrix],\n                     input_length=20,\n                     trainable=True))\nmodel.add(SpatialDropout1D(0.3))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\nmodel.add(GRU(300, dropout=0.3, recurrent_dropout=0.3))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(3))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n\n# Fit the model with early stopping callback\n\n#model.fit(xtrain_pad, y=ytrain_enc, batch_size=512, epochs=100, \n          #verbose=1, validation_data=(xvalid_pad, yvalid_enc), callbacks=[earlystop])","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:05:38.655013Z","iopub.execute_input":"2021-05-27T15:05:38.65541Z","iopub.status.idle":"2021-05-27T15:05:39.149925Z","shell.execute_reply.started":"2021-05-27T15:05:38.655375Z","shell.execute_reply":"2021-05-27T15:05:39.148624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=50, epochs=80 )","metadata":{"execution":{"iopub.status.busy":"2021-05-27T15:06:24.526306Z","iopub.execute_input":"2021-05-27T15:06:24.526724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is my bleu solution evaluation code","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nimport spacy\n\n\n\ndef convert_to_machine(test_input):\n    nlp = spacy.load(\"en_core_web_sm\")\n    my_list = \"\"\n    doc = nlp(test_input)   \n    for token in doc:        \n        lemm = token.lemma_\n        poss = token.pos_\n        stopp = token.is_stop\n        thisdict = {\n            \"lem\": lemm,\n            \"pos\": poss,\n            \"stop\": stopp\n        }\n        if (thisdict[\"stop\"] == False) & (thisdict[\"pos\"] != 'PUNCT') & (thisdict[\"pos\"] != 'SYM'):\n            my_list += thisdict[\"lem\"] + ' '\n       \n    return my_list\n#test 1\nbeing_tested = 'User shoot dm look forward'\ntest1 = convert_to_machine('good how are you')\ntest2 = convert_to_machine('how can I help you')\ntest3 = convert_to_machine('please send us a dm we look forward to helping you')\n\nscore1 = sentence_bleu(test1, being_tested)\nscore2 = sentence_bleu(test2, being_tested)\nscore3 = sentence_bleu(test3, being_tested)\n\nlist1 = [score1, score2, score3]\nres_max1 = max(list1,key=lambda x:float(x))\nprint('test 1', res_max1)\n\n#test 2\nbeing_tested = 'User happy help way start want receive detail link hyperlink'\ntest1 = convert_to_machine('We’re happy to help send us your details ')\ntest2 = convert_to_machine('Sorry to hear that, can you come into a store so we can look at it')\ntest3 = convert_to_machine('What ios version are you running?')\n\nscore1 = sentence_bleu(test1, being_tested)\nscore2 = sentence_bleu(test2, being_tested)\nscore3 = sentence_bleu(test3, being_tested)\n\nlist1 = [score1, score2, score3]\nres_max2 = max(list1,key=lambda x:float(x))\nprint('test 2',res_max2)\n\n#test 3\nbeing_tested = 'User forward enjoy flight sabi'\ntest1 = convert_to_machine('We can absolutely help you, please dm us your details so we can assist you.')\ntest2 = convert_to_machine('Forward your details to our dms. ')\ntest3 = convert_to_machine('Yes we can please enjoy your flight.')\n\nscore1 = sentence_bleu(test1, being_tested)\nscore2 = sentence_bleu(test2, being_tested)\nscore3 = sentence_bleu(test3, being_tested)\n\nlist1 = [score1, score2, score3]\nres_max3 = max(list1,key=lambda x:float(x))\nprint('test 3',res_max3)\n\n#test 4\nbeing_tested = 'User welcome anytime end'\ntest1 = convert_to_machine('You are welcome. We’re happy to help anytime.')\ntest2 = convert_to_machine('Thank you let us know if you have any further queries.')\ntest3 = convert_to_machine('Please reach out to us if you have any further concerns.')\n\nscore1 = sentence_bleu(test1, being_tested)\nscore2 = sentence_bleu(test2, being_tested)\nscore3 = sentence_bleu(test3, being_tested)\n\nlist1 = [score1, score2, score3]\nres_max4 = max(list1,key=lambda x:float(x))\nprint('test 4',res_max4)\n\n#test 5\nbeing_tested = 'User need assist complete service address need help hyperlink'\ntest1 = convert_to_machine('User loreen kindly dm book number attach guest passport copy fix there! - anwar')\ntest2 = convert_to_machine('Do you need help?')\ntest3 = convert_to_machine('We’re sorry to hear that, can you please dm us about your service today?')\n\nscore1 = sentence_bleu(test1, being_tested)\nscore2 = sentence_bleu(test2, being_tested)\nscore3 = sentence_bleu(test3, being_tested)\n\nlist1 = [score1, score2, score3]\n\nres_max5 = max(list1,key=lambda x:float(x))\nprint('test 5',res_max5)\n\nBLEU_score = res_max1 + res_max2 + res_max3 + res_max4 + res_max5\nBLEU_score = BLEU_score / 5\nprint('Blue score', BLEU_score)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T21:48:40.602422Z","iopub.execute_input":"2021-06-02T21:48:40.602909Z","iopub.status.idle":"2021-06-02T21:48:56.274025Z","shell.execute_reply.started":"2021-06-02T21:48:40.602868Z","shell.execute_reply":"2021-06-02T21:48:56.272822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is my solution evaluation using an LSA score","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n\ndef convert_to_machine(test_input):\n    nlp = spacy.load(\"en_core_web_sm\")\n    my_list = \"\"\n    doc = nlp(test_input)   \n    for token in doc:        \n        lemm = token.lemma_\n        poss = token.pos_\n        stopp = token.is_stop\n        thisdict = {\n            \"lem\": lemm,\n            \"pos\": poss,\n            \"stop\": stopp\n        }\n        if (thisdict[\"stop\"] == False) & (thisdict[\"pos\"] != 'PUNCT') & (thisdict[\"pos\"] != 'SYM'):\n            my_list += thisdict[\"lem\"] + ' '\n       \n    return my_list\n#test 1\nbeing_tested = 'User shoot dm look forward end'\ntest1 = convert_to_machine('good how are you')\ntest2 = convert_to_machine('how can I help you')\ntest3 = convert_to_machine('please send us a dm we look forward to helping you')\n\nbeing_tested = nlp(being_tested)\ntest1 = nlp(test1)\ntest2 = nlp(test2)\ntest3 = nlp(test3)\n\nsimil1 = being_tested.similarity(test1)\nsimil2 = being_tested.similarity(test2)\nsimil3 = being_tested.similarity(test3)\n\nlist1 = [simil1, simil2, simil3]\nres_max1 = max(list1,key=lambda x:float(x))\nprint(res_max1)\n\n#test 2\nbeing_tested = 'User happy help way start want receive detail link hyperlink end'\ntest1 = convert_to_machine('We’re happy to help send us your details ')\ntest2 = convert_to_machine('Sorry to hear that, can you come into a store so we can look at it')\ntest3 = convert_to_machine('What ios version are you running?')\n\nbeing_tested = nlp(being_tested)\ntest1 = nlp(test1)\ntest2 = nlp(test2)\ntest3 = nlp(test3)\n\nsimil1 = being_tested.similarity(test1)\nsimil2 = being_tested.similarity(test2)\nsimil3 = being_tested.similarity(test3)\n\nlist1 = [simil1, simil2, simil3]\nres_max2 = max(list1,key=lambda x:float(x))\nprint(res_max2)\n\n#test 3\nbeing_tested = 'User forward enjoy flight sabi end'\ntest1 = convert_to_machine('We can absolutely help you, please dm us your details so we can assist you.')\ntest2 = convert_to_machine('Forward your details to our dms. ')\ntest3 = convert_to_machine('Yes we can please enjoy your flight.')\n\nbeing_tested = nlp(being_tested)\ntest1 = nlp(test1)\ntest2 = nlp(test2)\ntest3 = nlp(test3)\n\nsimil1 = being_tested.similarity(test1)\nsimil2 = being_tested.similarity(test2)\nsimil3 = being_tested.similarity(test3)\n\nlist1 = [simil1, simil2, simil3]\nres_max3 = max(list1,key=lambda x:float(x))\nprint(res_max3)\n\n#test 4\nbeing_tested = 'User welcome anytime end'\ntest1 = convert_to_machine('You are welcome. We’re happy to help anytime.')\ntest2 = convert_to_machine('Thank you let us know if you have any further queries.')\ntest3 = convert_to_machine('Please reach out to us if you have any further concerns.')\n\nbeing_tested = nlp(being_tested)\ntest1 = nlp(test1)\ntest2 = nlp(test2)\ntest3 = nlp(test3)\n\nsimil1 = being_tested.similarity(test1)\nsimil2 = being_tested.similarity(test2)\nsimil3 = being_tested.similarity(test3)\n\nlist1 = [simil1, simil2, simil3]\nres_max4 = max(list1,key=lambda x:float(x))\nprint(res_max4)\n\n#test 5\nbeing_tested = 'User need assist complete service address need help hyperlink end'\ntest1 = convert_to_machine('Sorry to hear that, how can we help you?')\ntest2 = convert_to_machine('Do you need help?')\ntest3 = convert_to_machine('We’re sorry to hear that, can you please dm us about your service today?')\n\nbeing_tested = nlp(being_tested)\ntest1 = nlp(test1)\ntest2 = nlp(test2)\ntest3 = nlp(test3)\n\nsimil1 = being_tested.similarity(test1)\nsimil2 = being_tested.similarity(test2)\nsimil3 = being_tested.similarity(test3)\n\nlist1 = [simil1, simil2, simil3]\nres_max5 = max(list1,key=lambda x:float(x))\nprint(res_max5)\n\nLSA_score = res_max1 + res_max2 + res_max3 + res_max4 + res_max5\nLSA_score = LSA_score / 5\nprint('lsa score: ', LSA_score)","metadata":{"execution":{"iopub.status.busy":"2021-06-02T21:54:14.475346Z","iopub.execute_input":"2021-06-02T21:54:14.475801Z","iopub.status.idle":"2021-06-02T21:54:30.897197Z","shell.execute_reply.started":"2021-06-02T21:54:14.475763Z","shell.execute_reply":"2021-06-02T21:54:30.896286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below are some of my failed experiments, I wanted to separate them from my main project to not risk cluttering the kaggle document. I'll try to comment the separate projects but it's a bit of a mess. I don't expect most of this to run 100%.","metadata":{}},{"cell_type":"markdown","source":"This was some of my experimenting with word vectors and gensim as an alternative to the k-means algorithm. ","metadata":{}},{"cell_type":"code","source":"import gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\ndef sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\n\npre_data = question_f['processing_inti'].values\n\ndata = list(sent_to_words(pre_data))\n\n#print(data)\nid2word = corpora.Dictionary(data)\n\ntexts = data\n\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n#print(corpus)\n#[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n\n\n\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=20, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)\n\n#print(lda_model.print_topics())\ndoc_lda = lda_model[corpus]\n\n# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)\n\n\ndef format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n# Show\ndf_dominant_topic.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This was the RNN model I orginally intended to use, if I could've gotten it to work effectively.","metadata":{}},{"cell_type":"code","source":"\t\t\nimport tensorflow as tf\nimport tensorflow.compat.v1 as v1\ndef model_inputs():\n    '''Create palceholders for inputs to the model'''\n    input_data = v1.placeholder(tf.int32, [None, None], name='input')\n    targets = v1.placeholder(tf.int32, [None, None], name='targets')\n    lr = v1.placeholder(tf.float32, name='learning_rate')\n    keep_prob = v1.placeholder(tf.float32, name='keep_prob')\n\n    return input_data, targets, lr, keep_prob\n\timport tensorflow as tf\nimport tensorflow.compat.v1 as v1\ndef model_inputs():\n    '''Create palceholders for inputs to the model'''\n    input_data = v1.placeholder(tf.int32, [None, None], name='input')\n    targets = v1.placeholder(tf.int32, [None, None], name='targets')\n    lr = v1.placeholder(tf.float32, name='learning_rate')\n    keep_prob = v1.placeholder(tf.float32, name='keep_prob')\n\n    return input_data, targets, lr, keep_prob\n\tdef encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length, attn_length):\n    '''Create the encoding layer'''\n    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n    cell = tf.contrib.rnn.AttentionCellWrapper(drop, attn_length, state_is_tuple = True)\n    enc_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell,\n                                                   cell_bw = enc_cell,\n                                                   sequence_length = sequence_length,\n                                                   inputs = rnn_inputs, \n                                                   dtype=tf.float32)\n\n    return enc_state\ndef encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, sequence_length, attn_length):\n    '''Create the encoding layer'''\n    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n    drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n    cell = tf.contrib.rnn.AttentionCellWrapper(drop, attn_length, state_is_tuple = True)\n    enc_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n    _, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = enc_cell,\n                                                   cell_bw = enc_cell,\n                                                   sequence_length = sequence_length,\n                                                   inputs = rnn_inputs, \n                                                   dtype=tf.float32)\n\n    return enc_state\ndef decoding_layer_infer(encoder_state, dec_cell, dec_embeddings, start_of_sequence_id, end_of_sequence_id,\n                         maximum_length, vocab_size, decoding_scope, output_fn, keep_prob):\n    '''Decode the prediction data'''\n    infer_decoder_fn = tf.contrib.seq2seq.simple_decoder_fn_inference(\n        output_fn, encoder_state, dec_embeddings, start_of_sequence_id, end_of_sequence_id, maximum_length, vocab_size)\n    infer_logits, _, _ = tf.contrib.seq2seq.dynamic_rnn_decoder(dec_cell, infer_decoder_fn, scope=decoding_scope)\n    return infer_logits\ndef decoding_layer(dec_embed_input, dec_embeddings, encoder_state, vocab_size, sequence_length, rnn_size,\n                   num_layers, vocab_to_int, keep_prob, attn_length):\n    '''Create the decoding cell and input the parameters for the training and inference decoding layers'''\n    \n    with tf.variable_scope(\"decoding\") as decoding_scope:\n        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n        drop = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob = keep_prob)\n        cell = tf.contrib.rnn.AttentionCellWrapper(drop, attn_length, state_is_tuple = True)\n        dec_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n        \n        weights = tf.truncated_normal_initializer(stddev = 0.1)\n        biases = tf.zeros_initializer()\n        output_fn = lambda x: tf.contrib.layers.fully_connected(x, \n                                                                vocab_size, \n                                                                None, \n                                                                scope=decoding_scope,\n                                                                weights_initializer = weights,\n                                                                biases_initializer = biases)\n\n        train_logits = decoding_layer_train(\n            encoder_state[0], dec_cell, dec_embed_input, sequence_length, decoding_scope, output_fn, keep_prob)\n        decoding_scope.reuse_variables()\n        infer_logits = decoding_layer_infer(encoder_state[0], dec_cell, dec_embeddings, vocab_to_int['<GO>'],\n                                            vocab_to_int['<EOS>'], sequence_length, vocab_size,\n                                            decoding_scope, output_fn, keep_prob)\n\n    return train_logits, infer_logits\nfrom tensorflow.python.compiler import tensorrt as trt\n\ndef seq2seq_model(input_data, target_data, keep_prob, batch_size, sequence_length, source_vocab_size, target_vocab_size,\n                  enc_embedding_size, dec_embedding_size, rnn_size, num_layers, vocab_to_int, attn_length):\n    \n    '''Use the previous functions to create the training and inference logits'''\n    \n    enc_embed_input = trt.contrib.layers.embed_sequence(input_data, source_vocab_size+1, enc_embedding_size)\n    enc_state = encoding_layer(enc_embed_input, rnn_size, num_layers, keep_prob, sequence_length, attn_length)\n\n    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size+1, dec_embedding_size], -1.0, 1.0))\n    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n\n    train_logits, infer_logits = decoding_layer(dec_embed_input, dec_embeddings, enc_state, target_vocab_size+1, \n                                                sequence_length, rnn_size, num_layers, vocab_to_int, keep_prob, \n                                                attn_length)\n    \n    return train_logits, infer_logits\n# Set the parameters\n\n\nepochs = 100\nbatch_size = 128\nrnn_size = 512\nnum_layers = 2\nencoding_embedding_size = 512\ndecoding_embedding_size = 512\nattn_length = 10\nlearning_rate = 0.0005\nkeep_probability = 0.8\nmax_line_length = 30\n\ntrain_graph = tf.Graph()\nwith train_graph.as_default():\n    \n    # Load the model inputs\n    input_data, targets, lr, keep_prob = model_inputs()\n    # Sequence length will be the max line length for each batch\n    sequence_length = max_line_length\n    input_shape = tf.shape(input_data)\n    \n    # Create the logits from the model\n    train_logits, inference_logits = seq2seq_model(\n        tf.reverse(input_data, [-1]), targets, keep_prob, batch_size, sequence_length, len(source_vocab_to_int), \n        len(target_vocab_to_int), encoding_embedding_size, decoding_embedding_size, rnn_size, num_layers, \n        target_vocab_to_int, attn_length)\n    \n    # Create a tensor to be used for making predictions.\n    tf.identity(inference_logits, 'logits')\n    with tf.name_scope(\"optimization\"):\n        # Loss function\n        cost = tf.contrib.seq2seq.sequence_loss(\n            train_logits,\n            targets,\n            tf.ones([input_shape[0], sequence_length]))\n\n        # Optimizer\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n\n        # Gradient Clipping\n        gradients = optimizer.compute_gradients(cost)\n        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n        train_op = optimizer.apply_gradients(capped_gradients)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some testing of word2vec.","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nimport re\n\nvocab = []\nfor word in tokenizer.word_index:\n    vocab.append( word )\n\ndef tokenize( sentences ):\n    tokens_list = []\n    vocabulary = []\n    for sentence in sentences:\n        sentence = sentence.lower()\n        sentence = re.sub( '[^a-zA-Z]', ' ', sentence )\n        tokens = sentence.split()\n        vocabulary += tokens\n        tokens_list.append( tokens )\n    return tokens_list , vocabulary\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing the glove embedding and some different implementations.","metadata":{}},{"cell_type":"code","source":"max_features = 10000\nmaxlen = 20\n\nX_train, X_test, y_train, y_test = train_test_split(questions, answers, test_size=0.2,random_state=42)","metadata":{"execution":{"iopub.status.busy":"2021-05-26T20:19:25.758152Z","iopub.execute_input":"2021-05-26T20:19:25.758655Z","iopub.status.idle":"2021-05-26T20:19:25.784869Z","shell.execute_reply.started":"2021-05-26T20:19:25.758621Z","shell.execute_reply":"2021-05-26T20:19:25.783394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(X_train)\ntokenized_train = tokenizer.texts_to_sequences(X_train)\nX_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T18:25:19.810018Z","iopub.execute_input":"2021-05-24T18:25:19.810565Z","iopub.status.idle":"2021-05-24T18:25:19.937026Z","shell.execute_reply.started":"2021-05-24T18:25:19.810531Z","shell.execute_reply":"2021-05-24T18:25:19.936232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_test = tokenizer.texts_to_sequences(X_test)\nX_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T18:25:22.774676Z","iopub.execute_input":"2021-05-24T18:25:22.775252Z","iopub.status.idle":"2021-05-24T18:25:22.796675Z","shell.execute_reply.started":"2021-05-24T18:25:22.775216Z","shell.execute_reply":"2021-05-24T18:25:22.795559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMBEDDING_FILE = '/kaggle/input/glove840b300dtxt/glove.840B.300d.txt'\n\ndef get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","metadata":{"execution":{"iopub.status.busy":"2021-05-27T14:32:42.052406Z","iopub.execute_input":"2021-05-27T14:32:42.052946Z","iopub.status.idle":"2021-05-27T14:37:01.592457Z","shell.execute_reply.started":"2021-05-27T14:32:42.052899Z","shell.execute_reply":"2021-05-27T14:37:01.591294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2021-05-24T18:25:27.083765Z","iopub.execute_input":"2021-05-24T18:25:27.084159Z","iopub.status.idle":"2021-05-24T18:25:34.170264Z","shell.execute_reply.started":"2021-05-24T18:25:27.084127Z","shell.execute_reply":"2021-05-24T18:25:34.169011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#EMBEDDING_FILE = '/kaggle/input/glove840b300dtxt/glove.840B.300d.txt'\n\nembeddings_index = {}\nf = open('../input/glove840b300dtxt/glove.840B.300d.txt', errors = 'ignore', encoding='utf8')\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2021-05-26T20:05:29.785286Z","iopub.execute_input":"2021-05-26T20:05:29.78772Z","iopub.status.idle":"2021-05-26T20:05:30.0344Z","shell.execute_reply.started":"2021-05-26T20:05:29.787629Z","shell.execute_reply":"2021-05-26T20:05:30.031738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v / np.sqrt((v ** 2).sum())","metadata":{"execution":{"iopub.status.busy":"2021-05-24T18:05:38.46672Z","iopub.execute_input":"2021-05-24T18:05:38.467113Z","iopub.status.idle":"2021-05-24T18:05:38.478181Z","shell.execute_reply.started":"2021-05-24T18:05:38.46708Z","shell.execute_reply":"2021-05-24T18:05:38.477394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain_glove = [sent2vec(x) for x in tqdm(X_train)]\nxvalid_glove = [sent2vec(x) for x in tqdm(X_test)]","metadata":{"execution":{"iopub.status.busy":"2021-05-24T18:05:39.829967Z","iopub.execute_input":"2021-05-24T18:05:39.830511Z","iopub.status.idle":"2021-05-24T18:05:39.951708Z","shell.execute_reply.started":"2021-05-24T18:05:39.830477Z","shell.execute_reply":"2021-05-24T18:05:39.949691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing of tfidvectorizer.","metadata":{}},{"cell_type":"code","source":"from gensim import utils\nfrom gensim.models import Doc2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\ndef get_tfdi(text1):\n    tfidf = vectorizer.fit_transform([text1])\n    return text1","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.220281Z","iopub.status.idle":"2021-05-20T16:55:46.220725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Tfidf_scores = list()\nfor s in questions:\n    score = get_tfdi(s)\n    #print(s)\n    #print(score)\n    Tfidf_scores.append(score)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.22165Z","iopub.status.idle":"2021-05-20T16:55:46.222063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(Tfidf_scores)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.222871Z","iopub.status.idle":"2021-05-20T16:55:46.223284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Testing of word vectors and learning what they are.","metadata":{}},{"cell_type":"code","source":"import gensim.models\n\nsentences = questions_tokened\nmodel = gensim.models.Word2Vec(sentences=sentences, vector_size=300)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.224166Z","iopub.status.idle":"2021-05-20T16:55:46.224604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vec_customer = model.wv['customer']\n#print(vec_customer)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.22545Z","iopub.status.idle":"2021-05-20T16:55:46.225872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.wv.most_similar(positive=['customer'])","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.226825Z","iopub.status.idle":"2021-05-20T16:55:46.227239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('customer', 'bad', model.similarity('customer', 'bad'))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.228066Z","iopub.status.idle":"2021-05-20T16:55:46.228502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model = word2vec_model()\nw2v_model.save('word2vec_model')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.22946Z","iopub.status.idle":"2021-05-20T16:55:46.229879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('customer', 'bad', w2v_model.similarity('customer', 'bad'))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.230773Z","iopub.status.idle":"2021-05-20T16:55:46.231193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Another method for seq2seq modelling.","metadata":{}},{"cell_type":"code","source":"sentences = list(question_f['processing_inti'].values)\nanswers_pre =  list(question_f['processing_resp'].values)\nmax_length = 21\nmin_length = 2\nquestions = list()\nanswers = list()\nfor sent in sentences:\n    length = len(sent.split())\n    #print(len(tokens))\n    if length > min_length & length < max_length:\n        index = sentences.index(sent)\n        questions.append(sentences[index])\n        answers.append(answers_pre[index])\n        \nprint(len(questions))\nprint(len(answers))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.232161Z","iopub.status.idle":"2021-05-20T16:55:46.232618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nnp.random.seed(42)\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom keras.models import Model\nfrom keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\nfrom keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import Callback\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.233636Z","iopub.status.idle":"2021-05-20T16:55:46.234049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alternative method to preping the data for the seq2seq model featuring a different method to build a vocab.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nX = questions\ny = answers\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nmax_features = 10000\nmaxlen = 20\nembed_size = 300\n\nthreshold = 0.35\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts((X_train) + (X_test))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)\nx_test = np.asarray(x_test).astype('float32')\nX_train = np.asarray(X_train).astype('float32')\nX_train = np.asarray(X_train).astype('float32')\ny_train = np.asarray(y_train).astype('float32')\ny_test = np.asarray(y_test).astype('float32')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.234925Z","iopub.status.idle":"2021-05-20T16:55:46.235358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_a = {}\ndef build_vocaba(potential):\n    for line in potential:\n        for word in line.split():\n            if word not in vocab_a:\n                vocab_a[word] = 1\n            else:\n                vocab_a[word] += 1\n\nbuild_vocaba(answers)\n\nthreshold = 2\ncount = 0\nfor k,v in vocab_a.items():\n    if v >= threshold:\n        count += 1\n        \nprint(\"Size of total vocab:\", len(vocab_a))\nprint(\"Size of vocab we will use:\", count)\n\ndecoder_vocab = len(vocab_a)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.236261Z","iopub.status.idle":"2021-05-20T16:55:46.236729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_q = {}\ndef build_vocabq(potential):\n    for line in potential:\n        for word in line.split():\n            if word not in vocab_q:\n                vocab_q[word] = 1\n            else:\n                vocab_q[word] += 1\n\nbuild_vocabq(questions)\n\nthreshold = 2\ncount = 0\nfor k,v in vocab_q.items():\n    if v >= threshold:\n        count += 1\n        \nprint(\"Size of total vocab:\", len(vocab_q))\nprint(\"Size of vocab we will use:\", count)\n\nencoder_vocab = len(vocab_q)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.237524Z","iopub.status.idle":"2021-05-20T16:55:46.237942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoder_input = layers.Input(shape=(None,))\nencoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(\n    encoder_input\n)\n\n# Return states in addition to output\noutput, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(\n    encoder_embedded\n)\nencoder_state = [state_h, state_c]\n\ndecoder_input = layers.Input(shape=(None,))\ndecoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(\n    decoder_input\n)\n\n# Pass the 2 states to a new LSTM layer, as initial state\ndecoder_output = layers.LSTM(64, name=\"decoder\")(\n    decoder_embedded, initial_state=encoder_state\n)\noutput = layers.Dense(10)(decoder_output)\n\nmodel = keras.Model([encoder_input, decoder_input], output)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.238907Z","iopub.status.idle":"2021-05-20T16:55:46.239339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\n\nmodel.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=\"sgd\",\n    metrics=[\"accuracy\"],\n)\n\nmodel.fit(\n    X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=10\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.240392Z","iopub.status.idle":"2021-05-20T16:55:46.24082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alternative method for preparing the data, and a model at the end.","metadata":{}},{"cell_type":"markdown","source":"I will now prepare the tokenized data ready for the seq2seq\nFirst I prepared encoder_input_data:\n1. Determining the maximum length of the question sentences that's max_input_length.\n2. Padding the tokenized_que_lines to the max_input_length.\n3. Determining the vocabulary size ( num_que_tokens ) ","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers , activations , models , preprocessing , utils\n\n#question_lines = question_f['processing_inti']\n\ntokenizer = preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts( questions ) \ntokenized_que_lines = tokenizer.texts_to_sequences( questions )\n\nlength_list = list()\nfor token_seq in tokenized_que_lines:\n    length_list.append( len( token_seq ))\nmax_input_length = np.array( length_list ).max()\nprint( 'Question max length is {}'.format( max_input_length ))\n\npadded_ques_lines = preprocessing.sequence.pad_sequences( tokenized_que_lines , maxlen=max_input_length , padding='post' )\nencoder_input_data = np.array( padded_ques_lines )\nprint( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n\nque_word_dict = tokenizer.word_index\nnum_que_tokens = len( que_word_dict )+1\nprint( 'Number of Question tokens = {}'.format( num_que_tokens))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.241714Z","iopub.status.idle":"2021-05-20T16:55:46.242131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next decoder_input_data:\n1. Append '<'START'>' and '<'END'>' to each of the answer sentences.\n2. Same steps as before","metadata":{}},{"cell_type":"code","source":"#pre_answer_lines = question_f['processing_resp']\nanswer_lines = list()\n\nfor line in answers:\n    answer_lines.append( '<START' + line + '<END>' )\n    \ntokenizer = preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts( answer_lines ) \ntokenized_ans_lines = tokenizer.texts_to_sequences( answer_lines ) \n\nlength_list = list()\nfor token_seq in tokenized_ans_lines:\n    length_list.append( len( token_seq ))\nmax_output_length = np.array( length_list ).max()\nprint( 'Answer max length is {}'.format( max_output_length ))\n\npadded_ans_lines = preprocessing.sequence.pad_sequences( tokenized_ans_lines , maxlen=max_output_length, padding='post' )\ndecoder_input_data = np.array( padded_ans_lines )\nprint( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n\nans_word_dict = tokenizer.word_index\nnum_ans_tokens = len( ans_word_dict )+1\nprint( 'Number of Answer tokens = {}'.format( num_ans_tokens))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.243244Z","iopub.status.idle":"2021-05-20T16:55:46.243691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"create a vocab and get size","metadata":{}},{"cell_type":"code","source":"tokenizer = preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts( answers + questions )\nVOCAB_SIZE = len( tokenizer.word_index )+1\nprint( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.244541Z","iopub.status.idle":"2021-05-20T16:55:46.244956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Prepare decoder_target_data:\n1. remove '<'START'>' tokens\n2. one-hot encoding ","metadata":{}},{"cell_type":"code","source":"decoder_target_data = list()\nfor token_seq in tokenized_ans_lines:\n    decoder_target_data.append( token_seq[ 1 : ] ) \n    \npadded_ans_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\nonehot_ans_lines = utils.to_categorical( padded_ans_lines , VOCAB_SIZE )\ndecoder_target_data = np.array( onehot_ans_lines )\nprint( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.24585Z","iopub.status.idle":"2021-05-20T16:55:46.246272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nencoder_input = layers.Input(shape=(None,))\nencoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(\n    encoder_input\n)\n\n# Return states in addition to output\noutput, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(\n    encoder_embedded\n)\nencoder_state = [state_h, state_c]\n\ndecoder_input = layers.Input(shape=(None,))\ndecoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(\n    decoder_input\n)\n\n# Pass the 2 states to a new LSTM layer, as initial state\ndecoder_output = layers.LSTM(64, name=\"decoder\")(\n    decoder_embedded, initial_state=encoder_state\n)\noutput = layers.Dense(10)(decoder_output)\n\nmodel = keras.Model([encoder_input, decoder_input], output)\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.247209Z","iopub.status.idle":"2021-05-20T16:55:46.247659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers.experimental import preprocessing\n\ndata = questions\nlayer = preprocessing.TextVectorization()\nlayer.adapt(data)\nvectorized_textq = layer(data)\nprint(vectorized_textq)\n\ndata = answers\nlayer = preprocessing.TextVectorization()\nlayer.adapt(data)\nvectorized_texta = layer(data)\nprint(vectorized_texta)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.248556Z","iopub.status.idle":"2021-05-20T16:55:46.24897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = vectorized_textq\ny = vectorized_texta\n\nX_train, X_test, y_train, y_test = train_test_split(vectorized_textq, vectorized_texta, test_size=0.2, random_state=42)\n\nmodel = build_model(allow_cudnn_kernel=True)\nmodel.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=\"sgd\",\n    metrics=[\"accuracy\"],\n)\n\nmodel.fit(\n    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=10\n)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.249871Z","iopub.status.idle":"2021-05-20T16:55:46.250286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More model sequencing.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential()\n# Add an Embedding layer expecting input vocab of size 1000, and\n# output embedding dimension of size 64.\nmodel.add(layers.Embedding(input_dim=1000, output_dim=64))\n\n# Add a LSTM layer with 128 internal units.\nmodel.add(layers.LSTM(128))\n\n# Add a Dense layer with 10 units.\nmodel.add(layers.Dense(10))\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.251186Z","iopub.status.idle":"2021-05-20T16:55:46.251637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seq2seq_model_builder(HIDDEN_DIM=300):\n    \n    encoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n    encoder_embedding = embed_layer(encoder_inputs)\n    encoder_LSTM = LSTM(HIDDEN_DIM, return_state=True)\n    encoder_outputs, state_h, state_c = encoder_LSTM(encoder_embedding)\n    \n    decoder_inputs = Input(shape=(MAX_LEN, ), dtype='int32',)\n    decoder_embedding = embed_layer(decoder_inputs)\n    decoder_LSTM = LSTM(HIDDEN_DIM, return_state=True, return_sequences=True)\n    decoder_outputs, _, _ = decoder_LSTM(decoder_embedding, initial_state=[state_h, state_c])\n    \n    # dense_layer = Dense(VOCAB_SIZE, activation='softmax')\n    outputs = TimeDistributed(Dense(VOCAB_SIZE, activation='softmax'))(decoder_outputs)\n    model = Model([encoder_inputs, decoder_inputs], outputs)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.252441Z","iopub.status.idle":"2021-05-20T16:55:46.25286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, LSTM, Dense\n\n# Define an input sequence and process it.\nencoder_inputs = Input(shape=(None, num_que_tokens))\nencoder = LSTM(VOCAB_SIZE, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None, num_ans_tokens))\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the \n# return states in the training model, but we will use them in inference.\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n                                     initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.253655Z","iopub.status.idle":"2021-05-20T16:55:46.254062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, LSTM, Dense, Embedding\n# Define an input sequence and process it.\nencoder_inputs = Input(shape=(None,))\nx = Embedding(num_que_tokens, VOCAB_SIZE)(encoder_inputs)\nx, state_h, state_c = LSTM(VOCAB_SIZE,\n                           return_state=True)(x)\nencoder_states = [state_h, state_c]\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\nx = Embedding(num_ans_tokens, VOCAB_SIZE)(decoder_inputs)\nx = LSTM(VOCAB_SIZE, return_sequences=True)(x, initial_state=encoder_states)\ndecoder_outputs = Dense(num_decoder_tokens, activation='softmax')(x)\n\n# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\n# Compile & run training\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.255091Z","iopub.status.idle":"2021-05-20T16:55:46.255558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nencoder_inputs = tf.keras.layers.Input(shape=( max_input_length , ))\nencoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\nencoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True  )( encoder_embedding )\nencoder_states = [ state_h , state_c ]\n\ndecoder_inputs = tf.keras.layers.Input(shape=( max_output_length ,  ))\ndecoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\ndecoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True)\ndecoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\ndecoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \noutput = decoder_dense ( decoder_outputs )\n\nmodel = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.256405Z","iopub.status.idle":"2021-05-20T16:55:46.256846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=50, epochs=150 )\n#model.save( 'model.h5' ) ","metadata":{"execution":{"iopub.status.busy":"2021-05-20T16:55:46.257777Z","iopub.status.idle":"2021-05-20T16:55:46.258199Z"},"trusted":true},"execution_count":null,"outputs":[]}]}